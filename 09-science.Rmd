---
header-includes:
- \usepackage{amssymb}
- \usepackage{pifont}
- \usepackage{colortbl}
- \usepackage{xcolor}
- \usepackage{float}
output: pdf_document
---
# Critiquing scientific claims

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, comment=NA, fig.align='center', fig.width=5.5,fig.height=5.5)
```

To be credible, scientific claims must be able to survive
informed criticism.  The nature of the critique that is
needed will vary, depending on what is in question.
It will vary, at a broader level, depending on whether
what is in question is

* Measurements 
    + e.g., as in astronomy, distances to other planets, 
    stars, and galaxies
* or the results of an experiment 
   + e.g., showing that plants grow better when compost is 
   added to a nutrient deficient soil
* or a theory 
    + e.g., Newton's law of gravity
    
Measurements and experimental results must be replicable 
--- i.e., another experimenter must be able to repeat the 
experiment and obtain the same results.  Theories must 
be able to make successful predictions.

In areas where results depend on the sharing of data and
skills between different scientists and groups of scientists,
the critique that authors provide to the work of their fellow
authors will commonly ensure that what is submitted for peer
review is soundly based.  

For experimental studies that are
designed to stand on their own, the past decade has seen the
emergence of worrying evidence that a large amount of
published work is, when put to the test, not replicable.
Current scientific publication processes do not, in general, 
handle experimentally based claims well.  

## What results can be trusted?

Scientific processes work best when claims made by one scientist
or group of scientists attract widespread interest and critique
from a wider group of scientists who understand the work well
enough to provide informed and incisive criticism.  This can be
an effective way to identify claims that have no sound basis.
Examples are the May 2020 Lancet and New England Journal of 
Medicine studies, claiming to be based on observational data,
arguing that use of the drug hydroxychloroquine as a treatment 
for Covid-19 was increasing patient deaths.  
Issues with these papers were quickly identified because they 
made claims that bore on an issue of major concern, and attracted 
attention from readers who carefully scrutinized their detailed 
statements. They were quickly retracted.  How much that has no
sound basis does that attract such attention, and is never
challenged?

Heavy reliance on the sharing of data and skills, and full use of 
the benefits that modern technology has to offer, have been vital 
to progress in such areas as earthquake science, the study of 
viruses and vaccines, modelling of epidemics, and climate science.
This sharing of data and skills, and use of modern technology, also
helps in the critique of what has been published earlier.
Areas where there has not been the same impetus for change are much 
more susceptible to the damage that arises from systems for funding 
and publishing science that encourage the formal publication of
what would better be treated as preliminary results --- a first
stab at an answer.  Publication of experimental results should not 
be a once-for-all event, but a staged process that moves from 
“this looks promising” to “has been independently replicated”, 
and to post=publication critique.

Publication does not of itself validate scientific claims,
Rather, as stated in @popper_1963

> Observations or experiments can be accepted as supporting a theory (or a hypothesis, or a scientific assertion) only if these observations or experiments are severe tests of the theory – or in other words, only if they result from serious attempts to refute the theory. 

### Sources of failure {-}

Fraud, though uncommon, happens more often than one might hope.
What is disturbing is the small number of scientists with
large numbers of papers that were retracted on account of
fraud.  How were they able to get away with publishing so
many papers, usually with fraudulent data, before the 
first identification of fraud that led to a checking of all
their work?  @ritchie2020science (pp67-68) cites, as an 
extreme example, the case of a Japanese anesthesiologist 
with 183 retracted papers.

More common are mistakes in data collection, unacknowledged 
sources of bias, hype, mistakes or biases in the handling of 
data and/or in data analysis, attaching a much higher
degree of certainty to statistical evidence than the results
warrant, and selection effects. 

What gets published can be strongly affected by selection
effects.  There may be selection of a subset of data where
there appears to be an effect of interest, choice of the
outcome variable and/or analysis approach that most nearly
gives the result that is wanted, and so on. In analysis of
data from experiments where two treatments are compared,
the common use of the arbitrary $p <= 0.05$ criterion 
(see the next subsection) as a cutoff for deciding what 
will be published has the inevitable effect of selecting out, 
in contexts where there was no difference of consequence
one in twenty of such results for publication.  This adds
to other selection effects.

## The case of Eysenck and his collaborators

At the time of his death in 1997, Eysenck was the living psychologist 
most frequently cited in the peer-reviewed scientific literature.
Much of his work was controversial in its time, with papers
containing "questionable data and results so dramatic they beggared 
belief" [@o2020famous].  He relied heavily on what has now been 
identified as heavily doctored data that was supplied to him by
German collaborator Grossarth-Maticek.  Particularly egregious was
the claim that individuals with an identifiably cancer-prone personality 
had a risk of dying from cancer that was as much as 121 times higher 
than that of people with a “healthy” personality --- one of several 
links that the duo claimed to have found between personality and
mortality.  Investigations into Eysenck's work, including 
collaborative work with Grossarth-Maticek, are ongoing.  Fourteen 
papers have been retracted, and another 71 have received 
"expressions of concern".  A large replication study conducted in 
2004 found none of the claimed links, apart from a modest link between 
personality and cardiovascular disease.^[See further @CraigEtAl.]

In a book published two years before his death, Eysenck made
comments that provide an intriguing insight into his thinking.

> Scientists have extremely high motivation to succeed in discovering the truth; their finest and most original discoveries are rejected by the vulgar mediocrities filling the ranks of orthodoxy. .. . . The figures don’t quite fit, so why not fudge them a little bit to confound the infidels and unbelievers? Usually the genius is right, of course, and we may in retrospect excuse his childish games, but clearly this cannot be regarded as a licence for non-geniuses to foist their absurd beliefs on us. [-@eysenck1995genius, p.197]

Eysenck was alluding to claims that Newton had manipulated data, and 
suggesting that it was excusable for other "geniuses" to do the same.

## Detection of Covid-19 from chest images

@roberts2021common identified an astonishing 320 papers and preprints
that appeared between 1 January 2020 to 3 October 2020, and which
describe the use of new machine learning models for the diagnosis of
COVID-19 from chest radiographic (CXR) and chest computed tomography 
(CT) images.  Quality screening reduced this number to 62, which were
then examined in more detail. None of the 62 satisfied these more
detailed requirements, designed to check whether the algorithms used
had been shown to be effective for use in clinical practice.  Among
other deficiencies, 48 did not complete any external validation, and 
55 had a high risk of bias with respect to at least one of 
participants, predictors, outcomes and analysis. In an account
of the results that appeared in New Scientist, @roberts2021machine 
comments that, relative
to persisting to develop a model that will survive a rigorous 
checking process and might be used in practice, "it is far easier
to develop a model with poor rigour and [apparent] excellent
performance and publish this."  This is a damning indictment of
the way that large parts of the research and publication process
currently work.  The public good would be much better served
by a process that encourages researchers to persist until it
has been demonstrated that researchers have a model that meets
standards such as are set out in @roberts2021common.  One may
hope that one result of this work will be to shift the research
and publication focus accordingly.

## Laboratory studies --- what do we find?

In the past several years, there has been a steady accumulation
of evidence that relates to the claim, in @r19_ioannidis_2005,
that "most published research findings are false".  Ioannidis
has in mind, not published results in general, but primarily
laboratory studies.  Papers that have added to the 
body of evidence that broadly support claims made in the 
Ioannidis paper include:

* Amgen: Reproduced 6 only of 53 'landmark' cancer studies.
    + @r23_begley_ellis_2012
    + @r2_begley_2013 notes issues with the studies that failed
* Bayer: Main results from 19 of 65 'seminal' drug studies
    + NB, journal impact factor was not a good predictor!
    + @r9_prinz_schlange_asadullah_2011
* fMRI studies: 57 of 134 papers (42%) had $\geq$ 1 case lacking
check on separate test image. 
Another 14%, unclear ...
    + @r8_kriegeskorte_simmons_bellgowan_baker_2009
   
The psychological science community is further advanced in
addressing these issues that many other communities, with 
The Center for Open Science (COS) taking a strong lead in 
studies designed to document the extent of the issues.

Other Center for Open Science (COS) Projects have been:

* Many Labs --- reproduce 13 classical psych studies
    + Of  13 studies --- 10: successful, 1: weakly, 2: no!
    + Plots show scatter across the 36 participating teams 
    + @r7_klein_others_2014
* Cancer Studies --- 50 "most impactful" from 2010-2012
    + @r5_kaiser_2015

Details from other relevant studies are given in the
recent book @ritchie2020science "Science fictions: Exposing 
fraud, bias, negligence and hype in science."
Problems arise, primarily, in areas where relatively small 
groups of scientists, with similar training and skills, work
independently.  The critiques have limited relevance to
areas where the nature of the work forces collaboration
between scientists with diverse skills, widely across
different research groups.

The peer review process does at least impose some minimal
checks on what is published. In some cases, issues may
be identified subsequent to publication.  Replication
studies will in some cases provide clues on the reliance 
that can be placed on previously published results in the 
relevant area of research.  The case is at least better
than for claims made by those who promote "alternative
medicines", nowadays often on the internet, offering 
"evidence" that is largely anecdotal.

### The Reproducibility: Psychology project {-}

```{r cap10, echo=FALSE}
cap10 <- "Psychology reproducibility project --- Effect sizes are
compared between the replication and the initial study."
```

```{r effect-size, echo=FALSE, message=FALSE, warning=FALSE, fig.width=7.5, fig.height=7.5, fig.pos='H', out.width="70%", fig.cap=cap10}
# if(!exists('MASTER'))
# MASTER <- read.csv("~/_Tasks/repPsychSci/rpp_data.csv")[1:167, ]
# colnames(MASTER)[1] <- "ID" # Change first column name to ID to be able to load .csv file
# useHere <- !is.na(MASTER$T_r..O.) & !is.na(MASTER$T_r..R.)
# rpp_df <- MASTER[useHere, c('Journal..O.','Discipline..O.', 'Power..R.','T_r..O.','T_r..R.')]
# rpp_df <- droplevels(rpp_df)
# ord <- order(rpp_df$T_r..O.)
# rpp_df <- rpp_df[ord,]
# rpp_df$Power <- as.numeric(as.character(rpp_df$Power))
# names(rpp_df)[1:3] <- c('Journal','Discipline', 'Power')
load("data/rpp_df.RData")
library(mgcv, quietly=TRUE)
library(latticeExtra, quietly=TRUE)
gph <- xyplot(T_r..R. ~ T_r..O., data=rpp_df, groups=Discipline,
     xlab = list("Effect size r (original)", cex=1.25), alpha=0.75,
     ylab = list("Effect size r (replication)", cex=1.25),
     xlim = c(-.2, 1), ylim = c(-.3, 1),
     xaxs="i", yaxs="i", aspect = 1, 
     auto.key=list(text=c(levels(rpp_df$Discipline),levels(rpp_df$Journal)),
                   points=TRUE, space="right"
                   ),
     scales=list(cex=1.0, tck=0.5),
     par.settings=simpleTheme(pch=c(16,16,c(3,5,4)), cex=1.5,
                              col=c(trellis.par.get()$superpose.symbol$col[1:3], rep("black",2))),
     main=list("Psychology: Open Science Collaboration Results",font=1)
)
gph2 <- xyplot(T_r..R. ~ T_r..O., data=rpp_df, groups=Journal,
     xlim = c(-.2, 1), ylim = c(-.3, 1),
     xaxs="i", yaxs="i", 
     par.settings=simpleTheme(pch=c(3,5,4), col="white", lwd=2, cex=1.25)
)
# Fitted curve
RO.gam <- gam(T_r..R. ~ Discipline+s(T_r..O., by=Discipline), 
              family=scat, data=rpp_df)
rpp_df[, c('fit','se')] <- predict(RO.gam, se=TRUE)
rpp_df <- within(rpp_df, {lower=fit-se
                  upper=fit+se})
gph+as.layer(gph2)+
  layer(data=subset(rpp_df, Discipline=="Social"), 
          col2 <- trellis.par.get()$superpose.symbol$col[2],
          llines(T_r..O., fit, col=col2),
          llines(T_r..O., lower, col=col2, lty=2),
          llines(T_r..O., upper, col=col2, lty=2),
        panel.abline(h=0, col="gray"),
        panel.abline(v=0, col="gray"),
        panel.abline(a=0, b=1, col="gray"))
```

Figure \@ref(fig:effect-size) summarizes evidence from
the Reproducibility: Psychology project [@r1_osc_EstRep2015].
The effect size is the difference between the two means that
are to be compared, divided by the pooled standard deviation
for the two groups.

* It chose 100 studies (3 journals, 2008)
* There was one replicate only of each study
* Of the 97 original studies that claimed to find a real effect,
this was replicated in 35 cases only.

Notice that the effect size is almost always smaller for
the replication.  A smooth curve, with confidence interval,
has been fitted for results in social psychology.  Notice 
that, for social psychology, it is only for an original 
effect size greater than 0.4 that one starts to see a 
positive correlation between the effect sizes for replicate 
and original.  

## Truths that special interests find inconvenient

Standards for published work show huge variation across
different areas of science.
The evidence that human induced greenhouse gas emissions 
are driving global warming has for the past two decades 
or more been overwhelming.  Fierce criticism of any weakness
in the evidence presented has, while delaying effective
action, helped ensure that published work in this area meets
unusually high standards. 

### Styles of argument {-}

The tobacco industry has made extensive use of "the science is not settled" 
arguments in its efforts to dismiss the evidence that smoking causes lung cancer.
The same PR firms, and the same researchers used to support these efforts
were later used in the attempt to undermine climate science.^[https://www.scientificamerican.com/article/tobacco-and-oil-industries-used-same-researchers-to-sway-public1/>]  
The goal has been to raise doubt, create confusion, and undermine the 
science.

In spite of the support that has been available from the fossil fuel
industry for research that is critical of mainstream climate science
research, no substantially different alternative account has emerged, 
and no climate change models have emerged that give results that are 
widely different from the consensus. An interesting case is that of
Richard Muller’s Berkeley Earth Surface Temperature Study (BEST),
with a large part of the funding coming from the right-wing billionaire 
Charles Koch,  known for funding climate skeptic groups such as the 
Heartland Institute.

Richard Muller had been known for his skepticism, in part supported
by legitimate scientific concerns.  He made headlines when he announced 
his acceptance of what climate scientists had already been saying 
for more than 15 years previous. 
> After years of denying global warming, physicist Richard Muller now says "global warming is real and humans are almost entirely the cause."^[https://courses.seas.harvard.edu/climate/eli/Courses/global-change-debates/Sources/Hockeystick-global-temperature/more/Richard-Muller/Muller-is-a-believer-Hallelujah.pdf>]

The broad sweep of work in climate science is, because it has survived
informed critique, and because of the diversity of contributing skills
and data, unusually secure.  Details, especially as they affect what may 
happen in individual countries, are subject to continual revision.

A particular issue is the role of the greenhouse gases,  and of their 
interactions with water vapour.  Much larger than their direct effect
is the contribution that comes from their warming of the air in which 
water vapour is present, allowing it to retain more vapour and trap 
more heat).  A standard denialist trump card has been to claim the 
authority of scientists who have standing in their own areas for the 
claim that the contribution of greenhouse gases is inconsequential 
relative to that of water 
vapour.^[See <https://west.web.unc.edu/climate-change/> and
<>]
The scientists involved are among a number who have fossil fuel 
industry links, and have been willing to allow themselves to be 
used as advocates for an industry that sees action on climate 
change as a threat.^[<https://insideclimatenews.org/news/12032015/leaked-email-reveals-whos-who-list-climate-denialists-merchants-of-doubt-oreskes-fred-singer-marc-morano-steve-milloy>]

### Big Pharma --- inconvenient data {-}

Between 1999 and 2019, opioid deaths in the United States increased
by a factor of six, to almost 50,000.  

A large contributor has been the increased use of prescription
opioids. Purdue Pharma stands out for its aggressive marketing of 
oxycodone, sold under the brand name OxyContin, arguing that 
concerns over addiction and other dangers from the drugs were 
overblown. In September 2019, Purdue Pharma declared bankruptcy, 
facing significant liability in OxyContin and opioid addiction 
lawsuits.  Details of settlements are still being worked in the 
courts.^[ <https://topclassactions.com/lawsuit-settlements/open-lawsuit-settlements/opioids/purdue-opioid-addiction-class-action-settlement/>]

Strict regulations govern, in most countries, the approval of 
prescription drugs. Purdue Pharma exploited the much more
limited control over off-label use of approved drugs, i.e.,
use for purposes for which they have not received formal
approval, and for a time stayed under the radar.

Another scandal that demonstrates how drug companies can
sometimes work their way around the approval process
concerns the drug Vioxx, likewise marketed as a 
painkiller.[@valentine2007timeline]  Concerns that the
drug might be increasing the risk of heart attacks began
to emerge in the months following its approval for use
in May 1999. By November 1, a study set up to investigate
these concerns reported 79 heart attacks out of 4000
among those taking the drug, as opposed to 41 among a
comparator group that was taking naproxen.  The drug
continued on the market as the evidence against Vioxx
strengthened further.  It was argued, with no evidence
to back this up, that naproxen likely had a protective 
effect.  In any case, why allow Vioxx to go to market
when naproxen was clearly carried less risk.

In September 2004,
when a colon-polyp prevention study showed that Vioxx
increased the risk of heart attack after 18 months,
Merck withdrew the drug.  A Lancet paper that was 
published later estimated that between 88,000 and 
140,000 Americans had heart attacks from taking 
Vioxx.^[@juni2004risk] The increased risk continued 
long after patients had ceased taking the drug.

### What can one trust? {-}

One may hope that checks on any new drug will be
stricter than was the case for Vioxx, that lessons
have been learned, that where in future drugs come
up for approval, checks will continue on possible 
long-term effects. With fringe and quack medicines,
there are no such checks.

One can take encouragement from the way in which
evidence of the type discussed, that one other drug
is doing more harm than good, has in the cases 
discussed finally come to light.  They illustrate
the importance of collecting the relevant data, and
of taking note of what the data have to say.

It has been interesting to follow approval processes
for Covid-19 vaccines.  At the time of writing, the
Pfizer, Astrazena and Moderna vaccines have had, in
addition to their testing in clinical trials,
extraordinary levels of testing in clinical practice,
with risks that are small relative to the small risk
that we take ever time we cross a busy road.
 
## Tricks used to dismiss established results

The headings, and some of commentary, are adapted from an article by 
Associate Professor Hassan Vally from La Trobe University, 
[that appeared in _The Conversation_](https://theconversation.com/5-ways-to-spot-if-someone-is-trying-to-mislead-you-when-it-comes-to-science-138814?utm_medium=email&utm_campaign=Latest%20from%20The%20Conversation%20for%20March%209%202021%20-%201883318379&utm_content=Latest%20from%20The%20Conversation%20for%20March%209%202021%20-%201883318379+CID_6009c8d7af2a01f376a88d0491598cfa&utm_source=campaign_monitor&utm_term=5%20ways%20to%20spot%20if%20someone%20is%20trying%20to%20mislead%20you%20when%20it%20comes%20to%20science)^[<https://theconversation.com/5-ways-to-spot-if-someone-is-trying-to-mislead-you-when-it-comes-to-science-138814?utm_medium=email&utm_campaign=Latest%20from%20The%20Conversation%20for%20March%209%202021%20-%201883318379&utm_content=Latest%20from%20The%20Conversation%20for%20March%209%202021%20-%201883318379+CID_6009c8d7af2a01f376a88d0491598cfa&utm_source=campaign_monitor&utm_term=5%20ways%20to%20spot%20if%20someone%20is%20trying%20to%20mislead%20you%20when%20it%20comes%20to%20science>]

1. "The ‘us versus them’ narrative"  
The powers that be are trying to deceive us."  Ask who is
making real attempt to deceive --- commercial interest groups,
peddlers of quack treatments, influence peddlers, . . .
2. ‘I’m not a scientist, but…’
Meaning perhaps: "I’m not a scientist, but that does not stop me
making an authoritative pronouncement that flies in the face of
established results."
Or: “I know what the science says, but I’m keeping an open mind”.  
Politicians are among the most frequent offenders.
3. Reference to ‘the science not being settled’  
There are of course times when the science is not settled, and when this is 
the case, one can expect scientists to openly argue different points of view
based on the evidence available.  Or, what has come to be accepted wisdom
may turn out to be wrong or in need of substantial revision.  But challenges
to the accepted wisdom have to be carefully argued, and themselves survive
informed critique.
4. Overly simplistic explanations  
Oversimplifications and generalizations are central to many anti-science 
arguments.  Science is often messy, complex and full of nuance. The truth 
can be harder to explain, and can sometimes sound less plausible, than a 
simple but incorrect explanation.  Use of simplistic statistical arguments
is common, e.g., fit a line to the cherry-picked years 1998-2008, ignoring
year to year correlation as well as influences that operate over longer time 
periods.
5. Cherry-picking  
One is not entitled to choose one study over another just because it aligns
with what you prefer to believe. This is not how science works.  
Not all studies are equal; some provide much stronger evidence than others.
The way that choices are made has to stand up to critical scrutiny.
