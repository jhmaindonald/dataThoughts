# Regression and Correlation {#sec:reg}

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, comment=NA, fig.align='center', fig.width=5.5,fig.height=5.5)
```

When two variables show a relationship, they are said to be correlated. Regression and correlation offer alternative, and complementary, perspectives on the relationship.[^07-regress-1] Nonsense correlations that arise where the third variable is time provide simple examples.[^07-regress-2]

[^07-regress-1]: The discussion will avoid the technicalities of alternative ways to measure correlation. The particular correlation measure that is used in the examples that follow is the product-moment or Pearson correlation, which relates directly to linear (straight line) regression.

[^07-regress-2]: Yule-Simpson type effects, discussed in Section \@ref(sec:yule1), are important in a regression context also.

## Correlation is not causation

Variable A may cause variable B. Or variable B may cause variable A. Or both A and B may be caused (or driven) by a third variable C.

The following are examples where causation likely goes in the other direction, or where a third variable is likely to be involved. Such examples help highlight how correlation can and cannot reasonably be interpreted.

1.  Children of parents who try to control eating are more likely to be overweight.
2.  Ice cream consumption & polio were closely correlated in the 1950s.
    -   Summer was when the virus thrived.

Cases where the third variable is time, as in Figure \@ref(fig:socAnti), are a fruitful source of examples of spurious correlations.[^07-regress-3] Is there a third factor, or is this just a chance relationship?

[^07-regress-3]: Figure \@ref(fig:socAnti) is one of many such examples that are available from\
    <http://www.tylervigen.com/spurious-correlations>.

```{r cap11}
cap11 <- "Sociology PhDs awarded (from US National Science 
Foundation data) vs Deaths from Anticoagulants.  Notice
that `Doctorates` and `Deaths` show a very similar pattern
of change from year to year."
```

```{r socAnti, fig.width=7.75, fig.asp=0.375, fig.show='hold', out.width="100%", fig.cap=cap11}
suppressPackageStartupMessages(library(latticeExtra))
trellis.par.set(theme)
phdDeaths <- data.frame(Year = 1999:2009,
Doctorates = c(572,617,566,547,597,580,536,579,576,601,664),
Deaths = c(17,39,39,27,44,46,29,42,47,52,78)
# Doctorates = c(601,598,572,617,566,547,597,580,536,579,576,601,664),
# Deaths <- c(54,46,42,50,43,41,46,39,37,45,45,41,54)
)
# trainDeaths <- data.frame(Year = 1999:2009,
# trainDeaths=c(76,74,76,87,66,59,63,60,55,52,46),
#   Oil=c(96,110,103,127,60,54,43,36,20,11,22))
# letVenom <- data.frame(Year=1999:2009,
#   Letters= c(9, 8, 11, 12, 11, 13, 12, 9, 9, 7, 9),
#   Deaths = c(6, 5, 5, 10, 8, 14, 10, 4, 8, 5, 6))
obj1 <- xyplot(Doctorates ~ Year, data=phdDeaths, type="b", xlab="",
               par.settings=simpleTheme(col=c("purple","black"),
                                pch=16:17, cex=1.25),ylim=c(528,690))
obj2 <- xyplot(Deaths ~ Year, data=phdDeaths, type="b", 
               ylab=list("Deaths", x=-0.8, y=0.5))
gph1 <- update(doubleYScale(obj1, obj2, add.ylab2=TRUE)+latticeExtra::
                layer(panel.key(c('Doctorates','Deaths'), corner=c(0,1),
                                 y=0.98, col=c("purple","black"),
                                 columns=2)))
gph2 <- xyplot(Doctorates ~ Deaths, data=phdDeaths, pch=16,
               ylab=list("Doctorates", x=0.8, y=0.5))
print(gph1, position=c(0,0,0.6,1), more=TRUE)
print(gph2, position=c(0.64,0,1,1)
      
      )
```

Another example is that, over 1997 to 2009, US Sociology PhDs awarded correlated strongly with worldwide non-commercial space launches. These correlations would seem to be the result of chance. Look at enough variable pairs, and such correlatiosn will sometimes appear.

## Regression to the mean

Tall fathers are likely to have tall sons, but shorter than themselves. Tall sons are likely to have tall fathers, but shorter than themselves. The data shown in Figure \@ref(fig:pearson) are from @pearson1903laws. The correlation between son's height and father's height is 0.5.[^07-regress-4]

[^07-regress-4]: Kahneman argues, perhaps too simplistically that as height is mainly due to genetic factors, and fathers share half of their genes with their sons, this is to be expected.

Notice that the points that are plotted show a a symmetrical elliptical shaped scatter about the mean (shown with a large solid dot in Panel A). This type of scatter is, strictly, required for uses of the correlation that will now be discussed.

```{r cap12}
cap12 <- "Tall fathers are likely to have tall sons, but shorter than themselves. 
Tall sons are likely to have tall fathers, but shorter than themselves."
```

```{r pearson, fig.height=3.15, fig.width=7.5, fig.pos='H', out.width="100%", fig.show="hold", fig.cap=cap12}
opar <- par(mar=c(3.1,3.1,2.1, 1.1), mgp=c(2.0, .5,0), fig=c(0,0.48,0,1))
ptCol <- adjustcolor('black', alpha=0.4)
load('data/Pearson.RData')
ptCol <- adjustcolor('black', alpha=0.4)
plot(Pearson[,1:2], col=ptCol, xlab="Father's height (in)", ylab="Son's height (in)", fg="gray")
avSon <- mean(Pearson$Son)
avFather <- mean(Pearson$Father)
points(avFather, avSon, cex=1.5, col=2, pch=16)
r <- cor(Pearson)[1,2]
text(60.5,77, paste("r =", round(r,2)))
Pearson.lm <- lm(Son~Father, data=Pearson)
b <- coef(Pearson.lm)
pc95 <- quantile(Pearson$Father, 0.95)
son95 <- quantile(Pearson$Son, 0.95)
pred95 <- b[1]+b[2]*pc95
abline(Pearson.lm, lwd=2, col=2)
abline(v=pc95, col=2)
abline(h=pred95, col=2)
abline(h=son95, col=2, lty=2)
pcSon <- round(100*with(Pearson, sum(Son<pred95)/nrow(Pearson)))
mtext(side=3, line=0.05, "95th percentile", col=2, at=pc95)
mtext(side=4, line=0.05, paste0(pcSon,"th percentile"), col=2, at=pred95, 
      adj=0.5, srt=90)
mtext(side=3, line=1.0, "A: Son's height, given father's height",
      adj=0, at=56)
##
par(fig=c(0.52,1,0,1), new=TRUE)
plot(Pearson[,1:2], col=ptCol, fg="gray")
points(avFather, avSon, cex=1.5, col=2, pch=16)
text(60.5,77, paste("r =", round(r,2)))
Pearson.lm2 <- lm(Father~Son, data=Pearson)
c2 <- coef(Pearson.lm2)
b2 <- c(-c2[1]/c2[2], 1/c2[2])
son95 <- quantile(Pearson$Son, 0.95)
predF95 <- c2[1]+c2[2]*son95
## abline(Pearson.lm, col=adjustcolor("red",alpha=0.4))
abline(b2, col="purple", lwd=2)  ## Regression of x on y
abline(h=son95, col="purple", lty=1, lwd=1)
abline(v=predF95, col="purple")
pc95 <- quantile(Pearson$Father, 0.95)
abline(v=pc95, col="purple",lty=2)
pcF <- round(100*with(Pearson, sum(Father<=predF95)/nrow(Pearson)))
mtext(side=3, line=1.0, "B: Father's height, given son's height",
      adj=0, at=56)
mtext(side=3, line=0.05, paste0(pcF,"th percentile"), col="purple", at=predF95)
mtext(side=4, line=0.05, paste0("95th percentile"), col="purple", at=son95, srt=90)
par(opar)
```

Consider a father whose height is 72 inches, which is the 95th percentile of heights for fathers. What is a best guess for the height of a son? One can read the predicted value off from the graph (the solid horizontal line in Panel A). Or use the regression equation. Or, reason thus:

-   If the correlation between father's height and son's height were 0, the best guess would be the mean for son's, i.e., 68.7 inches
-   If the correlation were 1, the son's height would be the 95th percentile of heights for sons, i.e., 73.3 inches.
-   But, as the correlation is 0.5, the expected height is 68.7 + 0.5$\times$(73.3-68.7) = 71.0 inches, i.e., start at the mean and move 0.5 of the distance up to the 95th percentile.

Now consider a son whose height is 73.3 inches (the 95th percentile for sons). The argument now goes:

-   The best estimate of the father's height is 67.7 + 0.5$\times$(73.3-67.7)=2.8 inches, i.e., start at the mean for fathers and move 0.5 of the distance up to the 95th percentile.

Galton's 1886 data, which predates Pearson's data, shows a 0.46 correlation between child height and the average of the parent height.

## NBA player points --- correlations decline over time

In Figure \@ref(fig:NBA), Panel A shows total points for 2016-2017 versus 1 year earlier, for players who competed in both seasons. The correlation is 0.83. Panel B is for 2016-2017 versus 5 years earlier, with the correlation now reducing to 0.41.

```{r cap12.5}
cap12.5 <- "As time progresses, correlation decreases, and regression to the mean increases.  For Panel A, the correlation is 0.83, while in Panel B it is 0.41."
```

```{r NBA, out.width="100%", fig.width=7.5, fig.height=3.5, fig.show="hold", fig.pos="H", fig.cap=cap12.5}
load("data/NBAplayer.RData")
m1516 <- with(NBAplayer, match(z15$Name,z16$Name,nomatch=0))
df1617 <- with(NBAplayer, data.frame(p1516=z15$TotalPoints[m1516>0], p1617=z16$TotalPoints[m1516]))
# cor(df1617)  # 0.83
gph1 <- xyplot(p1617~p1516, data=df1617, 
     xlab="Total points, 2015-2016", 
     ylab="Total points, 2016-2017", type=c("p","r"),
     main=list("A: 2016-2017 vs 2015-2016", font=1, y=0))
m1116 <- with(NBAplayer, match(z11$Name,z16$Name,nomatch=0))
df1116 <- with(NBAplayer, data.frame(p1112=z11$TotalPoints[m1116>0], p1617=z16$TotalPoints[m1116]))
# cor(df1116) ## 0.41
gph2 <- xyplot(p1617~p1112, data=df1116, 
     xlab="Total points, 2010-2011", 
     ylab="Total points, 2016-2017", type=c("p","r"),  
     main=list("B: 2016-2017 vs 2010-2011",font=1, y=0))
print(gph1, position=c(0,0,0.51,1), more=TRUE)
print(gph2, position=c(0.49,0,1,1))
```

The scatter of points increases as values increase, on both axes. Calculations of the type given in the previous section, based on the usual correlation measure, while giving more approximate results, are adequate for present purposes.

## Secrist's "The Triumph of Mediocrity in Business"

Horace Secrist's 1933 book The Triumph of Mediocrity in Business" was based on annual data for 1920 to 1930. Secrist took 73 different industries, in each case examined the ratios

> Profits:sales; Profits:assets; Expenses:sales; Expenses:assets

For each industry in 1920: he then split firms into 4 quartiles: top 25%, 2nd highest 25%, 2nd lowest 25%, lowest 25%.

-   Took average for each statistic, for each quartile, for each year.
-   Surprise, surprise, the best went, on average, down ...

> Complete freedom to enter trade and the continuance of competition mean the perpetuation of mediocrity. ... neither superiority or inferiority will tend to persist. Rather, mediocrity tends to become the rule. [@secrist1933triumph]

```{r cap15}
cap15 <- "Secrist's data showed a correlation of 0.5 between
time intervals five years apart.  Panel A uses shows means of simulations,
starting with the four performance quartiles in 1920 and looking ahead.
Panel B starts with the equivalent quartiles in 1930, and looks back."
```

```{r sim, out.width="105%", fig.width=7.5, fig.asp=0.4, fig.cap=cap15, fig.pos='H'}
opar <- par(mfrow=c(1,2), mgp=c(2,0.5,0), mar=c(3.1,3.1,2.1,1.1))
set.seed(29)
x1 <- rnorm(1000, mean=34, sd=5)
x2 <- 0.7*x1 + rnorm(1000, mean=.3*34, sd=5*sqrt(1-.49))
x3 <- 0.7*x2 + rnorm(1000, mean=.3*34, sd=5*sqrt(1-.49))
## print(round(c(sd(x1),sd(x2),sd(x3)),2))
## print(round(c(range(x1),range(x2),range(x3)),2))
xx <- data.frame(x1=x1,x2=x2,x3=x3)
xx <- xx[order(xx[,1]),]
qxx <- sapply(xx, function(x)c(mean(x[1:250]), mean(x[251:500]),
 mean(x[501:750]),mean(x[751:1000])))
plot(c(1920,1925,1930),qxx[4,], xlab="", ylab="Profit:Sales", 
     ylim=c(26.6,40.7), fg="gray", type="n") 
for(i in 1:4){
lines(c(1920,1925,1930),qxx[i,], ylab="Profit", fg="gray", type="b", pch=16) 
}
text(rep(1920,4), qxx[,1], c("Lowest 25% in 1920","2nd Lowest 25% in 1920",
                             "2nd highest 25% in 1920", "Highest 25% in 1920"),
     pos=4, col=adjustcolor("blue",alpha=0.5))  
xx2 <- xx[order(xx[,3]),]
qxx2 <- sapply(xx2, function(x)c(mean(x[1:250]), mean(x[251:500]),
 mean(x[501:750]),mean(x[751:1000])))
mtext(side=3, line=0.75, "A: Forwards from 1920", adj=0, at=1918)
plot(c(1920,1925,1930),qxx[4,], xlab="", ylab="Profit:Sales", 
     ylim=c(26.6,40.7), fg="gray", type="n") 
for(i in 1:4){
lines(c(1920,1925,1930),qxx2[i,], ylab="Profit", fg="gray", type="b", pch=16) 
}
text(rep(1930,4), qxx2[,3], c("Lowest 25% in 1930","2nd Lowest 25% in 1930",
                             "2nd highest 25% in 1930", "Highest 25% in 1930"),
     pos=2, col=adjustcolor("blue",alpha=0.5))  
mtext(side=3, line=0.75, "B: Looking back from 1930",  adj=0, at=1918)
par(opar)
```

Secrist was seeing regression to the mean. Figure \@ref(fig:sim) makes the point that if one takes the four quartiles in 1930 and looks back to 1920, in each case there is a regression back to the mean. Given a correlation of 0.7 between time intervals five years apart, The absolute difference from the mean moves from 8 to 0.7$\times$ 8 (= `r 0.7*8`) to 0.7$\times$ 5.6 (= `r 0.7*5.6`), whether one moves by two successive five year intervals forward in time, or back in time.

### "Do old fallacies ever die?" {.unnumbered}

@smith-sd gives references to work by prominent economists in the past half-century that had quoted Secrist approvingly or repeated his error.

-   1980s investment textbook: "Ultimately, economic forces will force the convergence of the profitability and growth rates of different firms." This was backed up with a 1980/1966 Secrist type comparison.
-   2000: (Journal article) "... profitability is mean-reverting within as well as across industries. Other firms eventually mimic products and technologies that produce above normal profitability ..."
-   @wainerRegress cites other examples.

### Decathlon performances in 2006 {.unnumbered}

```{r Decath2006}
Decath2006 <- subset(GDAdata::Decathlon,yearEvent==2006)
## Check difference of rank from Pearson correlations
## corP <- cor(Decath2006[,(15:24)[ord]])
## corS <- cor(Decath2006[,(15:24)[ord]],method="spearman")
## round(corP-corS,2)
```

```{r GG2, eval=TRUE, echo=FALSE}
suppressPackageStartupMessages(library(ggplot2))
suppressPackageStartupMessages(library(GGally))
```

```{r gg, message=FALSE, results='hide', warning=FALSE, message=FALSE, fig.pos='H', eval=TRUE, cache=TRUE}
my_fn <- function(data, mapping, pts=list(), smt=list(), ...){
              ggplot(data = data, mapping = mapping, ...) +
                         do.call(geom_point, pts) +
                         do.call(geom_smooth, smt)
}
my_custom_cor <- function(data, mapping, color = I("grey50"), sizeRange = c(1, 5), ...) {

  # get the x and y data to use the other code
  x <- eval_data_col(data, mapping$x)
  y <- eval_data_col(data, mapping$y)
  ct <- cor.test(x,y)

  r <- unname(ct$estimate)
  rt <- format(r, digits=2)[1]

  # since we can't print it to get the strsize, just use the max size range
  cex <- max(sizeRange)

  # helper function to calculate a useable size
  percent_of_range <- function(percent, range) {
    percent * diff(range) + min(range, na.rm = TRUE)
  }

  # plot the cor value
  ggally_text(
    label = as.character(rt),
    mapping = aes(),
    xP = 0.5, yP = 0.5,
    size = I(percent_of_range(cex * abs(r), sizeRange)),
    color = color,
    ...
  )  +
    # remove all the background stuff and wrap it with a dashed line
    theme_classic() +
    theme(
      panel.background = element_rect(
        color = color,
        linetype = "longdash"
      ),
      axis.line = element_blank(),
      axis.ticks = element_blank(),
      axis.text.y = element_blank(),
      axis.text.x = element_blank()
    )
}
```

```{r cap13, echo=FALSE}
cap13 <- "Between event correlations for top performances in six
of the ten decathlon events in 2006.  Points that are plotted,
and correlations, are for times or distances achieved."
```

```{r selected, fig.width=5.5, fig.asp=0.85, results='hide', warning=FALSE, message=FALSE, fig.pos='H', out.width="68%", eval=TRUE, fig.cap=cap13}
colnam <- c("Polevault", "Javelin", "Discus", "Shotput", "Highjump", "Longjump")
ncols <- match(colnam, names(Decath2006))  #  c(11L, 12L, 10L, 6L, 7L, 5L)
ggpairs(Decath2006[, ncols],
        lower = list(continuous =
                       wrap(my_fn,
                            pts=list(size=0.25, colour=adjustcolor("black", alpha=0.4)),
                            smt=list(method="gam", se=F, size=0.5, colour="red"))),
        columnLabels=colnam,
        upper=list(continuous=wrap(my_custom_cor, sizeRange=c(3,5))))
```

```{r misc, eval=FALSE, echo=FALSE}
Decath2006 <- subset(GDAdata::Decathlon,yearEvent==2006)
ecdf(Decath2006[,6])(14.9)
## 0.9300292
longjMean <- mean(Decath2006[,5])
longjMean+(quantile(Decath2006[,5],.93)-longjMean)*0.29
## 6.97+0.29*(7.47-6.97)
## 7.115
```

Figure \@ref(fig:selected) shows between event correlations for top performances (6800 points and over) for six events in the 2006 decathlon.[^07-regress-5] Note the 0.032 correlation of javelin with high jump. Performance in one of these two sports was not a useful indicator of what to expect in the other.

[^07-regress-5]: The dataset `Decathlon` in the R package `GDAdata` has data for the 21-year period after new rules were introduced in 1985. See also the Estonian website <http://www.decathlon2000.com/>

The correlation between shot put distances and long jump distances is shown as a more informative 0.29. If we find that an athlete has achieved a distance of 14.9 meters in the shot put, which is at the 93$^{th}$ percentile across athletes as a whole (7% did better), a best estimate for the long jump can be obtained thus:

-   A 14.9 meter put is at the 93$^{th}$ centile (7% will do better)
-   The long jump mean is 6.97, with the 93% mark = 7.47
    -   The difference from the mean is 7.47-6.97 = 0.5
-   The estimate for the long jump is then 6.97+0.29 $\times$ 0.5 = 7.12 

## Moderating predictive assessments

### Moderating sales estimates {.unnumbered}

You are the sales forecaster for a department store chain. All stores are similar in size and merchandise offered, but random factors affect sales in any year. Overall sales are expected to increase by 10% from 2020 to 2021. Sales in 2020, with the expected total and mean for 2021 are, in millions of dollars:

| Store | 2020             | 2021 |
|-------|------------------|------|
| 1     | 10               | ---  |
| 2     | 23               | ---  |
| 3     | 18               | ---  |
| 4     | 29               | ---  |
| TOTAL | 80 t each of the |      |

stores in 2021. The mean sales amount in 2021 is predicted to be 22,000,000 dollars? With a correlation of 0.4, the predicted sales for the individual stores are obtained thus:

| Store | 2020 | Subtract 20 | Xply by 0.4, add to 22 | Predicted sales |
|-------|------|-------------|------------------------|-----------------|
| 1     | 10   | -10         | 22-4                   | 18              |
| 2     | 23   | +3          | 22+1.2                 | 23.2            |
| 3     | 18   | -2          | 22-0.8                 | 21.2            |
| 4     | 29   | +9          | 29+3.6                 | 32.6            |
| MEAN  | 20   | 0           | 22                     | 22              |

In the real world of 2020 and 2021, the Covid-19 pandemic makes all such predictions hazardous!

### Choosing from job applicants {.unnumbered}

Correlation between presentation & performance is likely to be lower for the less well-known. In both cases performance is likely, relative to presentation, to move in closer to the mean. For less well-known candidates, the shift towards the mean is likely to be greater.

### Kahneman's comments on regression to the mean {.unnumbered}

> "Extreme predictions and a willingness to predict rare events from weak evidence are both manifestations of System 1. ..."

> "Regression to the mean is also a problem for System 2. The very idea ... is alien and difficult to communicate and comprehend. This is a case where System 2 requires special training."

> "We intuitively want to match predictions to the evidence."

> "We will not learn to understand regression from experience."

### Regression to the mean in verse {.unnumbered}

<https://www.youtube.com/watch?v=sxMllckUWaw>

## Time per unit distance for hillraces

Regression coefficients may differ greatly depending on what 
adjustments are made for other variables.  This is important 
for attaching meaning to a coefficient.  For the hillrace
data that will now be considered, it is relatively easy to 
tease out the role of the explanatory variables that have 
been included in one or alternative versions of the regression 
equation. Especially where there are three or more explanatory
variables, with the manner in which they should enter into the
regression equation is unclear, the effect of an individual
variable that is of interest can be difficult or impossible
to tease out.

The hillrace dataset has record times for 23 Northern Ireland 
Mountain Running Association hillraces, as given in the 2007
calendar.  In the models fitted and graphs shown that follow,
the distance measure is `Dist` (distance converted to kilometers), 
the climb measure is `Climb` (vertical distance between lowest 
and highest point, in meters), and the time measure is `Time`
(in minutes).

How does time per unit distance (`timePerKm`) vary with distance. 
We will fit two equations, both with $y$ = `timePerKm`.

```{r coeffs, echo=FALSE}
nihr <- with(DAAG::nihills, 
             data.frame(minPerKm=time*60/(dist*1.6093),
                        Dist=dist*1.609344, Climb=climb*0.3048))
climb.lm <- lm(minPerKm~log(Dist)+log(Climb), data = nihr)
slope.lm <- lm(minPerKm~log(Dist)+log(Climb/Dist), data = nihr)
bClimb <- coef(climb.lm)
constCl <- c(bClimb[1]+bClimb[3]*mean(log(nihr$Climb)), bClimb[2])
bSlope <- coef(slope.lm)
constSl <- c(bSlope[1]+bSlope[3]*mean((log(nihr$Climb/nihr$Dist))), bSlope[2])
```

```{r cap1}
cap1 <- "Variation in time per unit distance with distance.  Panel A
shows the pattern of change when `log(Climb)` is held constant at its 
mean value, while Panel B shows the pattern of change when 
`log(Climb/Dist)` is held constant at its mean value."
```

```{r partial, fig.width=8, fig.asp=0.4, out.width="100%", fig.show="hold", fig.pos="H", fig.cap=cap1}
opar <- par(mfrow=c(1,2), mgp=c(2,0.5,0), mar=c(3.1,3.1,2.1,1.1))
lineCols <- c("red", adjustcolor("magenta",0.4))
car::crPlots(climb.lm, terms=.~log(Dist), xaxt='n', yaxt='n', 
             xlab="Dist", ylab="Minutes per kilometer", col.lines=lineCols)
mtext("A: Climb held constant at mean value", adj=0, line=0.8, at=1)
axis(2, at=4:7-mean(nihr$minPerKm), labels=paste(4:7))
labx <- c(4,8,16,32)
axis(1, at=log(2^(2:5)), labels=paste(2^(2:5)))
car::crPlots(slope.lm, terms=.~log(Dist), xaxt='n', yaxt='n', 
             xlab="Dist", ylab="Minutes per kilometer", col.lines=lineCols)
mtext("B: log(Climb/Dist) held constant at mean value", adj=0, line=0.8, at=1)
axis(1, at=log(2^(2:5)), labels=paste(2^(2:5)))
axis(2, at=4:7-mean(nihr$minPerKm), labels=paste(4:7))
par(opar)
```

Figure \@ref(fig:partial)A shows the dependence of
`timePerKm` depends on `log(Dist)`, when `log(Climb)`
is held at its mean value.  Use of `log(Dist)` rather than
`Dist` means that distance on the $x$-axis from 2 to 4 (km)
is the same as from 4 to 8, or from 8 to 16, or from 16 to 32,
i.e., equal distances correspond to equal multiplicative
changes.  The equation that is plotted is

> `timePerKm` = `r round(constCl[1],1)` `r round(constCl[2],2)` $\times$ `log(Dist)`

Figure \@ref(fig:partial)B shows the dependence of
`timePerKm` depends on `log(Dist)`, when `log(Climb/Dist)`
is held at its mean value. The equation that is plotted is

> `timePerKm` = `r round(constSl[1],2)` + `r round(constSl[2],2)` $\times$ `log(Dist)`


In Panel A, time per kilometer decreases quite sharply as distance
increases.  This happens because the ratio of `Climb` to `Dist` 
decreases if `Climb` is held constant while `Dist` increases, 
i.e., longer distance races involve gentler ascents and  descents.

Panel B shows what happens when `Climb/Dist` is held 
constant, i.e., we are comparing races with the same ratio of 
`Climb` to `Dist`.  As expected, time per kilometer does then 
decrease as distance increases.

## Does fitting a least squares line make sense?

### Are herricanes more dangerous than himmicanes? {-}

```{r cap17-5}
cap17.5 <- "Deaths versus damage estimate in US dollars. The red (for female) 
and blue (for male) dashed lines are close equivalents of Jung at al's fit
to the data. The $y$-axis uses a scale of equal relative numbers of deaths, 
while the\ $x$-axis uses a scale of equal dollar damage costs."
```

```{r hurricanes1, echo=FALSE, fig.width=6.0, fig.height=4.15, message=FALSE, fig.cap=cap17.5, fig.pos='H', out.width="80%", fig.align="center"}
hurricNamed <- within(DAAG::hurricNamed, lnNDAM2014<-log(NDAM2014))
suppressPackageStartupMessages(library(latticeExtra, quietly=TRUE))
suppressPackageStartupMessages(library(mgcv))
trellis.par.set(theme)
library(MASS)
ytxt <- c(1,3,10, 30, 100, 300, 1000)
funy <- function(y)log(y+1)
gph <- xyplot(funy(deaths) ~ NDAM2014,
       groups= mf,
       data=hurricNamed,
       scales=list(y=list(at=funy(ytxt), labels=paste(ytxt))),
       xlab =expression("Damage at the time (millions of 2014 US$)"),
       ylab=expression("Deaths"),
       auto.key=list(text=c("Females","Males"), columns=2),
       main=list("US Atlantic Hurricanes -- 1950 to 2012",font=1),
       par.settings=simpleTheme(col=c("red","blue"),
                                pch=16))
gph2 <- gph + latticeExtra::layer(panel.text(x[c(13,84)], y[c(13,84)],
                       labels=hurricNamed[c(13,84), "Name"], pos=3,col="gray30", cex=0.8),
                    panel.text(x[c(13,84)], y[c(13,84)],
                       labels=hurricNamed[c(13,84), "Year"], pos=1, col="gray30", cex=0.8))
ab <- coef(lm(funy(deaths) ~ NDAM2014 + mf, data=hurricNamed))
gph3 <- gph2+latticeExtra::layer(panel.abline(ab[1], b=ab[2], 
                                              col="red", lty=2, lwd=2),
          panel.abline(ab[1]+ab[3], b=ab[2], col="blue", lty=2, lwd=2))
# gph4 <- gph3 + latticeExtra::layer(data=df,
#            panel.lines(funx(NDAM2014[mf=='f']), hat[mf=='f'], col='red',
#                        lwd=2, lty=2, alpha=0.5),
#            panel.lines(funx(NDAM2014[mf=='m']),
#            hat[mf=='m'], col='blue', lwd=2, lty=2, alpha=0.5))
gph3 + latticeExtra::layer(lims <- current.panel.limits(),
             xleft <- lims$xlim[1] + 0.01*diff(lims$xlim),
             ytop <- lims$ylim[2] - 0.125*diff(lims$ylim),
             panel.text(xleft, ytop, adj=0,
                        paste("Dashed lines, red for femaleand blue for male,\n",
                              "are close equivalents to the Jung et al fits.\n"),
                        cex=0.8, col="gray30"))
```

The United States National Hurricane Center began formally naming
hurricanes in 1950, a task now under control of the
World Meteorological Organization.  Female names were used for
Atlantic hurricanes from 1953 to 1978, with a mix of male and
female names used from 1979 onwards.

In a paper titled "Female hurricanes are deadlier than male 
hurricanes", @jung2014female used data for 94 Atlantic hurricanes 
that made landfall in the United States during 1950-2012 to argue
that death rates from those with female names were overall higher 
than for those with male names. The suggestion was that where names
were female, authorities took the risk less seriously.  The paper 
attracted wide interest on the blogosphere, with female hurricanes 
jokingly called herricanes and males called himmicanes.

```{r cap18}
cap18 <- "Deaths versus damage estimate in US dollars, with logarithmic scales
               on both axes. Separate fitted lines for male and female
               hurricanes cannot be distinguished. Jung et al used a 
               logarithmic scale on the vertical axis only, which on
               this graph leads to the dashed curves."
```

```{r hurricanes2, echo=FALSE, fig.width=6.0, fig.height=4.15, message=FALSE, fig.cap=cap18, fig.pos='H', out.width="80%", fig.align="center"}
hurricNamed <- within(DAAG::hurricNamed, lnNDAM2014<-log(NDAM2014))
detach('package:GGally')
detach('package:ggplot2')
library(latticeExtra, quietly=TRUE)
suppressPackageStartupMessages(library(mgcv))
library(MASS)
ytxt <- c(1,3,10, 30, 100, 300, 1000)
xtxt <- c(1,10,100,1000, 10000, 100000, 1000000 )
funx <- function(x)log(x)
funy <- function(y)log(y+1)
gph <- xyplot(funy(deaths) ~ funx(NDAM2014),
       groups= mf, data=hurricNamed,
       scales=list(y=list(at=funy(ytxt), labels=paste(ytxt)),
                   x=list(at=funx(xtxt), labels=paste(xtxt))),
       xlab =expression("Damage at the time (millions of 2014 US$)"),
       ylab=expression("Deaths"),
       auto.key=list(text=c("Females","Males"), columns=2),
       main=list("US Atlantic Hurricanes -- 1950 to 2012",font=1),
       par.settings=simpleTheme(col=c("red","blue"),
                                pch=16))
gph2 <- gph + layer(panel.text(x[c(13,84)], y[c(13,84)],
                       labels=hurricNamed[c(13,84), "Name"], pos=3,col="gray30", cex=0.8),
                    panel.text(x[c(13,84)], y[c(13,84)],
                       labels=hurricNamed[c(13,84), "Year"], pos=1, col="gray30", cex=0.8))
ab <- coef(lm(funy(deaths) ~ funx(NDAM2014) + mf, data=hurricNamed))
curve.gam <- gam(funy(deaths)~s(lnNDAM2014) + mf, family=scat,
                 data=hurricNamed)
dfs <- expand.grid(mf=factor(c('f','m')),
                             lnNDAM2014=pretty(hurricNamed$lnNDAM2014, 25))
dfs[["hats"]] <- predict(curve.gam, newdata=dfs)
## gph3 <- gph2+layer(panel.abline(ab[1], b=ab[2], col="red", alpha=0.4),
##           panel.abline(ab[1]+ab[3], b=ab[2], col="blue", lty=2,
##                        alpha=0.4))
ord <- order(hurricNamed$NDAM2014)
gph3 <- gph2+as.layer(xyplot(hats~lnNDAM2014, groups=mf, lty=1, type="l", lwd=4,
                             alpha=0.4, data=dfs))
hurricJ <- hurricNamed[-(c(13,84)),]
curve.nb <- glm.nb(deaths~NDAM2014+mf, data=hurricJ)
x <- exp(log(with(hurricJ, pretty(funx(NDAM2014), 40)))/0.14)
df<- expand.grid(mf=factor(c('f','m')), NDAM2014=x)
df[,'hat'] <- funy(predict(curve.nb, newdata=df, type='response')-1)
gph4 <- gph3 + layer(data=df,
           panel.lines(funx(NDAM2014[mf=='f']), hat[mf=='f'], col='red',
                       lwd=2, lty=2, alpha=0.5),
           panel.lines(funx(NDAM2014[mf=='m']),
           hat[mf=='m'], col='blue', lwd=2, lty=2, alpha=0.5))
gph4 + layer(lims <- current.panel.limits(),
             xleft <- lims$xlim[1] + 0.01*diff(lims$xlim),
             ytop <- lims$ylim[2] - 0.125*diff(lims$ylim),
             panel.text(xleft, ytop, adj=0,
                        paste("Dashed curves translate the male &\n",
                              "female regression lines in the previous\n",
                              "figure to the present graph. The solid\n",
                              "curves show a data-based relationship."),
                        cex=0.8, col="gray30"))
```

The separate dotted lines in Figure \@ref(fig:hurricanes1), red for 
female and blue for male, are a close equivalent to the authors' fit 
to the data.  Notice the use of a relative (numbers of deaths) scale 
on the $y$-axis, and a dollar scale on the $x$-axis. 
An unfortunate consequence of the use of a linear dollar scale
on the $x$-axis is that the slopes of the lines are strongly
influenced by the final four points at the upper end of the scale.
Why did the authors not use, at least as a starting point, the same relative scale on both axes, as in Figure 
\@ref(fig:hurricanes2)?

As well as using a relative scale on the $x$-axis, Figure 
\@ref(fig:hurricanes2) uses a methodology that allows the 
data to determine the form of the response.  Deaths do on 
average increase more at a higher rate than the damage measure, 
but not at the rate suggested by the dashed curves. There
is now no evident difference between the two curves.

Jung et al omitted `Audrey` (in 1957) and `Katrina` (in 2005), as 
outliers. These are included in Figures \@ref(fig:hurricanes1) and \@ref(fig:hurricanes2), with the curves fitted using a "robust" 
fitting method that is relatively insensitive to outliers. 
Other differences between the Jung et al analysis, and the
analyses reflected in Figures \@ref(fig:hurricanes1) and 
\@ref(fig:hurricanes2) are documented in Note 1
`r if (knitr:::is_latex_output()) ', on p. \\pageref{hurricanes}'`

### Historical speed of light estimates --- is there a pattern?{-}

```{r c-data}
cvalues <- data.frame(
  Year = c(1675, 1729, 1849, 1862, 1907, 1926, 1950, 1958, 1972),
  speed = c(220000, 301000, 315000, 298000, 299710, 299796,
            299792.5, 299792.50, 299792.4562)/1000,
  error = c(NA, NA, NA, 500, 30, 4, 3, 0.1, 0.00111)/1000
)
```

```{r cap19}
cap19 <- "Successive speed of light estimates.  Panel B 
limits attention to measurements made in 1926 and later. 
The line was fitted with no adjustment for the very
different error estimates.  The dashed curve, which 
incorporates such adjustments, is statistically 
indistinguishable from the thick gray horizontal line."
```

```{r plot-c-data, fig.width=7.5, fig.height=3.0, fig.show='hold', fig.pos='H', fig.cap=cap19}
opar <- par(mar=c(3.1,4.1,2.1, 0.6), mgp=c(2.0, .5,0), fig=c(0,0.51,0,1))
plot(speed ~ Year, data=cvalues, cex=1.0, cex.lab=1.0, pch=1,
     xlab="", ylab="Speed (1000s of km/s)", fg="gray")
rect(1915,295.5,1980, 304, col="lightgray", border=NA)
with(cvalues, points(speed ~ Year, pch=16, cex=1.0))
obj <- lm(speed ~ Year, data=cvalues)
abline(obj)
mtext("A: All measurements", side=3, line=0.75, cex=1.15, at=1630, adj=0)
subdata <- subset(cvalues, Year>=1926)
ylim <- with(subdata, range(c(speed-error, speed+error), na.rm=TRUE))
par(fig=c(0.49,1,0,1), new=TRUE)
plot(speed ~ Year, data=subdata, ylim=ylim, pch=0, cex.lab=1.15,
     xlab="", ylab="", fg="gray")
obj <- lm(speed ~ Year, data=subdata)
abline(obj)
obj3 <- lm(speed ~ poly(Year,2), data=subdata, weights=error^-2)
obj4 <- lm(speed ~ 1, data=subdata, weights=error^-2)
lines(subdata$Year, fitted(obj3), lty=2)
lines(subdata$Year, fitted(obj4), col='gray', lwd=4)
mtext("B: From 1926, with confidence bands", side=3, line=0.75, at=1920, adj=0, cex=1.15)
with(subdata, segments(Year, speed - error, Year, speed +
     error))
with(subdata, segments(Year - 1.25, speed - error, Year +
     1.25, speed - error))
with(subdata, segments(Year - 1.25, speed + error, Year +
     1.25, speed + error))
par(opar)
```

Creationist Barry Setterfield has argued that a reduction over 
time in the speed of light has led the passage of time to slow 
down, relative to the remote past, so that the universe is 
thousands rather than billions of years old.  His arguments rely 
on making various adjustments to figures obtained historically,
selecting what he regarded as the most reliable data, and
then fitting a curve.  

Setterfield tells a story that, while a little different from that of 
the line in Panel A of Figure \@ref(fig:plot-c-data), makes equally 
little sense.  The right panel is limited to the points from 1926 
and on, marked off with the gray background on the left panel.^[Data 
are from <https://en.wikipedia.org/wiki/Speed_of_light>]

For the measurements from 1862 onward, estimates of accuracy
are available.  Until 1950, each new estimate lay outside the
bounds for the previous estimate, indicating that these
were underestimates.
Even if one were to accept Setterfield's manipulation of the data, 
it makes no sense at all to fit either lines such as are shown, or
curves, to data values which have such very different
accuracies.  

Even if one were to accept Setterfield's manipulation of the data, 
it makes no sense at all to fit either lines such as are shown, or
curves, to data values which have such very different
accuracies as those shown in the graphs.  For the 
measurements from 1862 onwards, estimates of accuracy are
available.  Until 1950, each new estimate lay outside the
bounds for the previous estimate, indicating that these
were underestimates.

### Global mean temperature trends

Figure \@ref(fig:climate) plots global 
[air and sea surface temperature anomaly data](https://iridl.ldeo.columbia.edu/SOURCES/.NASA/.GISS/.GISSTEMP/.Global/.LOTI/)
against year. Anomalies, in hundredths of a degree centigrade, are
differences from the 1951-1980 global average. The grey curve plots
the average anomaly up to that point in time. 

```{r cap20}
cap20 <- "Anomalies (differences) in hundredths of a degree centigrade
from global average temperatures over 1951-1980, plotted against year.
The gray curve shows, for each year, the average anomaly up to that
point in time.  The last year in which this lay below the gray line
was 1962."
```
  
```{r climate, fig.width=6.25, fig.asp=0.7, out.width="80%", fig.pos='H', fig.cap=cap20}
## ---- loti-smooth --------
load('data/loti.RData')
anomaly <- loti[, "J.D"]
num <- seq(along = anomaly)
AVtodate <- cumsum(anomaly)/num
yr <- loti$Year
anomTxt <- "Difference from baseline"
yl = substitute(txt * " (0.01" * degree * "C)",
                  list(txt=anomTxt))
plot(yr, anomaly, xlab = "Year", ylab = as.expression(yl), fg="gray")
mtext(side=3, line=0.75,
      "Global temperature differences from 1951-1980 global average")
lines(AVtodate ~ yr, col = "gray", lwd = 2)
lastLessYr <- max(yr[anomaly < AVtodate])
lastLessy <- loti[as.character(lastLessYr), "J.D"]
yarrow <- lastLessy - c(4, 0.75) * strheight("O")
arrows(lastLessYr, yarrow[1], lastLessYr, yarrow[2],
       col = "gray", lwd = 2)
```

Observe that 1964 was the last year in which the global 
temperature fell below the average to that time.
For the 52 subsequent years (from 1965 to 2016 inclusive),
the global average was above the average up to that date.  Under
the (false) assumption that global temperature is varying
randomly (and therefore independently) about a common mean,
the probability of this happening is 2$^{-40}$ = 9.1
$\times$ 10$^{-13}$.  A variation of this argument came
from a speaker on the Australian ABC Science Show on April
3 2011. 

Under any model that accounts for what are now
fairly well understood patterns of correlation over time,
the probability, while very small, is not that small!
Arguments that overstate the case for what is now a
well-established pattern of change are unhelpful

It is likewise nonsensical to fit a line to the cherry-picked
years 1998-2008, where the trend was relatively flat. Year
to year temperatures are correlated.


