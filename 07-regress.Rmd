# Regression and Correlation {#sec:reg}

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, comment=NA, fig.align='center', fig.width=5.5,fig.height=5.5)
```

When two variables show a relationship, they are said to be
correlated.  Regression and correlation offer alternative,
and complementary, perspectives on the relationship.^[The 
discussion will avoid the technicalities of alternative
ways to measure correlation.  The particular correlation 
measure that is used in the examples that follow is
the product-moment or Pearson correlation, which relates
directly to linear (straight line) regression.]
Nonsense correlations that arise where the third variable is 
time provide simple examples.^[Yule-Simpson type effects, 
discussed in Section \@ref(sec:yule1), are important in a 
regression context also.]

## Correlation is not causation

Variable A may cause variable B.  Or variable B may cause
variable A.  Or  both A and B may be caused (or driven) by
a third variable C.  

The following are examples where causation likely goes in the other
direction, or where a third variable is likely to be involved.
Such examples help highlight how correlation can and cannot 
reasonably be interpreted. 

1. Children of parents who try to control eating are more likely 
to be overweight.
2. Countries with higher IQs have higher average wealth measures.
3. Ice cream consumption & polio were closely correlated in the 1950s.
    + Summer was when the virus thrived.

Cases where the third variable is 
time, as in Figure \@ref(fig:socAnti), are a fruitful source
of examples of spurious correlations.^[Figure \@ref(fig:socAnti)
is one of many such examples that are available from  
<http://www.tylervigen.com/spurious-correlations>.]
Is there a third factor, or is this just a chance relationship?

```{r cap11}
cap11 <- "Sociology PhDs awarded (from US National Science 
Foundation data) vs Deaths from Anticoagulants.  Notice
that `Doctorates` and `Deaths` show a very similar pattern
of change from year to year.
"
```

```{r socAnti, fig.width=7.75, fig.asp=0.375, fig.show='hold', out.width="100%", fig.cap=cap11}
suppressPackageStartupMessages(library(latticeExtra))
trellis.par.set(theme)
phdDeaths <- data.frame(Year = 1999:2009,
Doctorates = c(572,617,566,547,597,580,536,579,576,601,664),
Deaths = c(17,39,39,27,44,46,29,42,47,52,78)
# Doctorates = c(601,598,572,617,566,547,597,580,536,579,576,601,664),
# Deaths <- c(54,46,42,50,43,41,46,39,37,45,45,41,54)
)
# trainDeaths <- data.frame(Year = 1999:2009,
# trainDeaths=c(76,74,76,87,66,59,63,60,55,52,46),
#   Oil=c(96,110,103,127,60,54,43,36,20,11,22))
# letVenom <- data.frame(Year=1999:2009,
#   Letters= c(9, 8, 11, 12, 11, 13, 12, 9, 9, 7, 9),
#   Deaths = c(6, 5, 5, 10, 8, 14, 10, 4, 8, 5, 6))
obj1 <- xyplot(Doctorates ~ Year, data=phdDeaths, type="b", xlab="",
               par.settings=simpleTheme(col=c("purple","black"),
                                pch=16:17, cex=1.25),ylim=c(528,690))
obj2 <- xyplot(Deaths ~ Year, data=phdDeaths, type="b", 
               ylab=list("Deaths", x=-0.8, y=0.5))
gph1 <- update(doubleYScale(obj1, obj2, add.ylab2=TRUE)+latticeExtra::
                layer(panel.key(c('Doctorates','Deaths'), corner=c(0,1),
                                 y=0.98, col=c("purple","black"),
                                 columns=2)))
gph2 <- xyplot(Doctorates ~ Deaths, data=phdDeaths, pch=16,
               ylab=list("Doctorates", x=0.8, y=0.5))
print(gph1, position=c(0,0,0.6,1), more=TRUE)
print(gph2, position=c(0.64,0,1,1)
      
      )
```


Another example is that, over 1997 to 2009, US Sociology PhDs
awarded correlated strongly with worldwide non-commercial 
space launches.  These correlations would seem to be the
result of chance.  Look at enough variable pairs, and such
correlatiosn will sometimes appear.

## Regression to the mean

Tall fathers are likely to have tall sons, but shorter than themselves. 
Tall sons are likely to have tall fathers, but shorter than themselves.
The data shown in Figure \@ref(fig:pearson) are from @pearson1903laws.
The correlation between son's height and father's height is 
0.5.^[Kahneman argues, perhaps too simplistically that as height is 
mainly due to genetic factors, and fathers share half of their genes
with their sons, this is to be expected.]

Notice that the points that are plotted show a a symmetrical 
oval shaped scatter about the mean (shown with a large solid 
dot in Panel A).  Thistype of scatter is, strictly, required 
for uses of the correlation that will now be discussed. 

Consider a father whose height is 72 inches, which is the 
95th percentile of heights for fathers. What is a best guess 
for the height of a son?

```{r cap12}
cap12 <- "Tall fathers are likely to have tall sons, but shorter than themselves. 
Tall sons are likely to have tall fathers, but shorter than themselves."
```

```{r pearson, fig.height=3.15, fig.width=7.5, fig.pos='H', out.width="100%", fig.show="hold", fig.cap=cap12}
opar <- par(mar=c(3.1,3.1,2.1, 1.1), mgp=c(2.0, .5,0), fig=c(0,0.48,0,1))
ptCol <- adjustcolor('black', alpha=0.4)
load('Data/Pearson.RData')
ptCol <- adjustcolor('black', alpha=0.4)
plot(Pearson[,1:2], col=ptCol, xlab="Father's height (in)", ylab="Son's height (in)", fg="gray")
avSon <- mean(Pearson$Son)
avFather <- mean(Pearson$Father)
points(avFather, avSon, cex=1.5, col=2, pch=16)
r <- cor(Pearson)[1,2]
text(60.5,77, paste("r =", round(r,2)))
Pearson.lm <- lm(Son~Father, data=Pearson)
b <- coef(Pearson.lm)
pc95 <- quantile(Pearson$Father, 0.95)
son95 <- quantile(Pearson$Son, 0.95)
pred95 <- b[1]+b[2]*pc95
abline(Pearson.lm, col=2)
abline(v=pc95, col=2)
abline(h=pred95, col=2)
abline(h=son95, col=2, lty=2)
pcSon <- round(100*with(Pearson, sum(Son<pred95)/nrow(Pearson)))
mtext(side=3, line=0.05, "95th percentile", col=2, at=pc95)
mtext(side=4, line=0.05, paste0(pcSon,"th percentile"), col=2, at=pred95, 
      adj=0.5, srt=90)
mtext(side=3, line=1.0, "Son's ht, given father's ht (red lines)",
      adj=0, at=56)
par(fig=c(0.52,1,0,1), new=TRUE)
plot(Pearson[,1:2], col=ptCol, fg="gray")
text(60.5,77, paste("r =", round(r,2)))
Pearson.lm2 <- lm(Father~Son, data=Pearson)
c2 <- coef(Pearson.lm2)
b2 <- c(-c2[1]/c2[2], 1/c2[2])
son95 <- quantile(Pearson$Son, 0.95)
predF95 <- c2[1]+c2[2]*son95
abline(Pearson.lm, col=adjustcolor("red",alpha=0.4))
abline(b2, col="purple", lty=2, lwd=2)
abline(h=son95, col="purple", lty=2, lwd=2)
abline(v=predF95, col="purple", lty=2, lwd=2)
abline(v=pc95, col=adjustcolor('red',alpha=0.4))
abline(h=pred95, col=adjustcolor('red',alpha=0.4))
pcF <- round(100*with(Pearson, sum(Father<=predF95)/nrow(Pearson)))
mtext(side=3, line=1.0, "Father's ht, given son's ht (purple lines)",
      adj=0, at=56)
mtext(side=3, line=0.05, paste0(pcF,"th percentile"), col="purple", at=predF95)
mtext(side=4, line=0.05, paste0("95th percentile"), col="purple", at=son95, srt=90)
par(opar)
```

* If the correlation between father's height and son's height were 0, 
the best guess would be the mean for son's, i.e., 68.7 inches
* If the correlation were 1, the son's height would be the
95th percentile of heights for sons, i.e., 73.3 inches.
* But, as the correlation is 0.5, the expected height is 
68.7 + 0.5$\times$(73.3-68.7) = 71.0 inches, i.e., start at
the mean and move 0.5 of the distance up to the 95th percentile.

Now consider a son whose height is 73.3 inches (the 95th percentile for 
sons):

* The best estimate of the father's height is 67.7 +
0.5$\times$(73.3-67.7)=2.8 inches,  i.e., start at the mean
for fathers and move 0.5 of the distance up to the 95th percentile.

Galton's 1886 data, which predates Pearson's data, shows a 0.46
correlation between child height and the average of the parent
height.

## NBA player points --- correlations decline over time

In Figure \@ref(fig:NBA), Panel A shows total points for 
2016-2017 versus 1 year earlier, for players who competed
in both seasons.  The correlation is 0.83. Panel B is
for 2016-2017 versus 5 years earlier, with the correlation 
now reducing to 0.41. 

```{r cap12.5}
cap12.5 <- "As time progresses, correlation decreases, and regression to the mean increases.  For Panel A, the correlation is 0.83, while in Panel B it is 0.41."
```

```{r NBA, out.width="100%", fig.width=7.5, fig.height=3.5, fig.show="hold", fig.pos="H", fig.cap=cap12.5}
load("data/NBAplayer.RData")
m1516 <- with(NBAplayer, match(z15$Name,z16$Name,nomatch=0))
df1617 <- with(NBAplayer, data.frame(p1516=z15$TotalPoints[m1516>0], p1617=z16$TotalPoints[m1516]))
# cor(df1617)  # 0.83
gph1 <- xyplot(p1617~p1516, data=df1617, 
     xlab="Total points, 2015-2016", 
     ylab="Total points, 2016-2017", type=c("p","r"),
     main=list("A: 2016-2017 vs 2015-2016", font=1, y=0))
m1116 <- with(NBAplayer, match(z11$Name,z16$Name,nomatch=0))
df1116 <- with(NBAplayer, data.frame(p1112=z11$TotalPoints[m1116>0], p1617=z16$TotalPoints[m1116]))
# cor(df1116) ## 0.41
gph2 <- xyplot(p1617~p1112, data=df1116, 
     xlab="Total points, 2010-2011", 
     ylab="Total points, 2016-2017", type=c("p","r"),  
     main=list("B: 2016-2017 vs 2010-2011",font=1, y=0))
print(gph1, position=c(0,0,0.51,1), more=TRUE)
print(gph2, position=c(0.49,0,1,1))
```
The scatter of points increases as values increase, on both axes.
Calculations of the type given in the previous section, based on 
the usual correlation measure, while giving more approximate results,
are adequate for present purposes.

## Secrist's "The Triumph of Mediocrity in Business"

Horace Secrist's 1933 book The Triumph of Mediocrity in Business" 
was based on annual data for 1920 to 1930.  Secrist took 73 different
industries, in each case examined the ratios

> Profits:sales; Profits:assets; Expenses:sales; Expenses:assets

For each industry in 1920: he then split firms into 4 quartiles: top 25%, 
2nd highest 25%, 2nd lowest 25%, lowest 25%.

* Took average for each statistic, for each quartile, for each year.
* Surprise, surprise, the best went, on average, down ...

> Complete freedom to enter trade and the continuance of competition
mean the perpetuation of mediocrity.  ...  neither superiority or
inferiority will tend to persist. Rather, mediocrity tends to become
the rule.

```{r cap15}
cap15 <- "Secrist's data showed a correlation of 0.5 between
time intervals five years apart.  Panel A uses shows means of simulations,
starting with the four performance quartiles in 1920 and looking ahead.
Panel B starts with the equivalent quartiles in 1930, and looks back."
```

```{r sim, out.width="105%", fig.width=7.5, fig.asp=0.4, fig.cap=cap15, fig.pos='H'}
opar <- par(mfrow=c(1,2), mgp=c(2,0.5,0), mar=c(3.1,3.1,2.1,1.1))
set.seed(29)
x1 <- rnorm(1000, mean=34, sd=5)
x2 <- 0.7*x1 + rnorm(1000, mean=.3*34, sd=5*sqrt(1-.49))
x3 <- 0.7*x2 + rnorm(1000, mean=.3*34, sd=5*sqrt(1-.49))
## print(round(c(sd(x1),sd(x2),sd(x3)),2))
## print(round(c(range(x1),range(x2),range(x3)),2))
xx <- data.frame(x1=x1,x2=x2,x3=x3)
xx <- xx[order(xx[,1]),]
qxx <- sapply(xx, function(x)c(mean(x[1:250]), mean(x[251:500]),
 mean(x[501:750]),mean(x[751:1000])))
plot(c(1920,1925,1930),qxx[4,], xlab="", ylab="Profit:Sales", 
     ylim=c(26.6,40.7), fg="gray", type="n") 
for(i in 1:4){
lines(c(1920,1925,1930),qxx[i,], ylab="Profit", fg="gray", type="b", pch=16) 
}
text(rep(1920,4), qxx[,1], c("Lowest 25% in 1920","2nd Lowest 25% in 1920",
                             "2nd highest 25% in 1920", "Highest 25% in 1920"),
     pos=4, col=adjustcolor("blue",alpha=0.5))  
xx2 <- xx[order(xx[,3]),]
qxx2 <- sapply(xx2, function(x)c(mean(x[1:250]), mean(x[251:500]),
 mean(x[501:750]),mean(x[751:1000])))
mtext(side=3, line=0.75, "A: Forwards from 1920", adj=0, at=1918)
plot(c(1920,1925,1930),qxx[4,], xlab="", ylab="Profit:Sales", 
     ylim=c(26.6,40.7), fg="gray", type="n") 
for(i in 1:4){
lines(c(1920,1925,1930),qxx2[i,], ylab="Profit", fg="gray", type="b", pch=16) 
}
text(rep(1930,4), qxx2[,3], c("Lowest 25% in 1930","2nd Lowest 25% in 1930",
                             "2nd highest 25% in 1930", "Highest 25% in 1930"),
     pos=2, col=adjustcolor("blue",alpha=0.5))  
mtext(side=3, line=0.75, "B: Looking back from 1930",  adj=0, at=1918)
par(opar)
```
Secrist was seeing regression to the mean. Figure \@ref(fig:sim)
makes the point that if one takes the four quartiles in 1930 and looks 
back to 1920, in each case there is a regression back to the mean.
Given a correlation of 0.7 between time intervals five years apart,
The absolute difference from the mean moves from 8 to 0.7$\times 8 
(= `r 0.7*8`) to 0.7$\times 5.6 (= `r 0.7*5.6`), whether one moves
forward in time, or back in time.

### "Do old fallacies ever die?" {-}

@smith-sd gives references to work by prominent economists in the past
half-century that had quoted Secrist approvingly or repeated his error.

* 1970: "The book [by Secrist] contains an elaborate statistical
demonstration that, over a period of time, initially high-performing ..."
* 1980s investment textbook: "Ultimately, economic forces will force
the convergence of the profitability and growth rates of 
different firms." This was backed up with a 1980/1966 
Secrist type comparison.
* 2000: (Journal article) "... profitability is mean-reverting
within as well as across industries.  Other firms eventually 
mimic products and technologies that produce above normal
profitability ..."
* @wainerRegress cites other examples.

### Decathlon performances in 2006 {-} 

```{r Decath2006}
Decath2006 <- subset(GDAdata::Decathlon,yearEvent==2006)
## Check difference of rank from Pearson correlations
## corP <- cor(Decath2006[,(15:24)[ord]])
## corS <- cor(Decath2006[,(15:24)[ord]],method="spearman")
## round(corP-corS,2)
```

```{r GG2, eval=TRUE, echo=FALSE}
suppressPackageStartupMessages(library(ggplot2))
suppressPackageStartupMessages(library(GGally))
```

```{r gg, message=FALSE, results='hide', warning=FALSE, message=FALSE, fig.pos='H', eval=TRUE, cache=TRUE}
my_fn <- function(data, mapping, pts=list(), smt=list(), ...){
              ggplot(data = data, mapping = mapping, ...) +
                         do.call(geom_point, pts) +
                         do.call(geom_smooth, smt)
}
my_custom_cor <- function(data, mapping, color = I("grey50"), sizeRange = c(1, 5), ...) {

  # get the x and y data to use the other code
  x <- eval_data_col(data, mapping$x)
  y <- eval_data_col(data, mapping$y)
  ct <- cor.test(x,y)

  r <- unname(ct$estimate)
  rt <- format(r, digits=2)[1]

  # since we can't print it to get the strsize, just use the max size range
  cex <- max(sizeRange)

  # helper function to calculate a useable size
  percent_of_range <- function(percent, range) {
    percent * diff(range) + min(range, na.rm = TRUE)
  }

  # plot the cor value
  ggally_text(
    label = as.character(rt),
    mapping = aes(),
    xP = 0.5, yP = 0.5,
    size = I(percent_of_range(cex * abs(r), sizeRange)),
    color = color,
    ...
  )  +
    # remove all the background stuff and wrap it with a dashed line
    theme_classic() +
    theme(
      panel.background = element_rect(
        color = color,
        linetype = "longdash"
      ),
      axis.line = element_blank(),
      axis.ticks = element_blank(),
      axis.text.y = element_blank(),
      axis.text.x = element_blank()
    )
}
```

```{r cap13, echo=FALSE}
cap13 <- "Between event correlations for top performances in six
of the ten decathlon events in 2006.  Points that are plotted,
and correlations, are for times or distances achieved."
```

```{r selected, fig.width=5.5, fig.asp=0.85, results='hide', warning=FALSE, message=FALSE, fig.pos='H', out.width="68%", eval=TRUE, fig.cap=cap13}
colnam <- c("Polevault", "Javelin", "Discus", "Shotput", "Highjump", "Longjump")
ncols <- match(colnam, names(Decath2006))  #  c(11L, 12L, 10L, 6L, 7L, 5L)
ggpairs(Decath2006[, ncols],
        lower = list(continuous =
                       wrap(my_fn,
                            pts=list(size=0.25, colour=adjustcolor("black", alpha=0.4)),
                            smt=list(method="gam", se=F, size=0.5, colour="red"))),
        columnLabels=colnam,
        upper=list(continuous=wrap(my_custom_cor, sizeRange=c(3,5))))
```

```{r misc, eval=FALSE, echo=FALSE}
Decath2006 <- subset(GDAdata::Decathlon,yearEvent==2006)
ecdf(Decath2006[,6])(14.9)
## 0.9300292
longjMean <- mean(Decath2006[,5])
longjMean+(quantile(Decath2006[,5],.93)-longjMean)*0.29
## 6.97+0.29*(7.47-6.97)
## 7.115
```

Figure \@ref(fig:selected) shows between event correlations for
top performances (6800 points and over) for six of the events
in the decathlon in
2006.^[Data for the twenty-one year period after new rules were
introduced in 1985 are available in the R package `GDAdata` with
the name `Decathlon`.  See also the Estonian website
<http://www.decathlon2000.com/>]  Observe that the correlation
of javelin with high jump is 0.032, so that performance in one
of these sports gives no useful indication of the performance
that might be expected in the other.

The correlation between shot put distances and long jump distances
is shown as a more informative 0.29.  If we find that an athlete
has achieved a distance of 14.9 meters in the shot put, which is 
at the 93$^{th}$ percentile across athletes as a whole 
(7% did better), a best estimate for the long jump can be 
obtained thus:

* A 14.9 meter put is at the 93$^{th}$ centile (7% will do better)
* The long jump mean is 6.97, with the 93% mark = 7.47
    + The difference from the mean is 7.47-6.97 = 0.5
* The estimate for the long jump is then 6.97+0.29 $\times$ 0.5 = 7.12
```

## Moderating predictive assessments

### Moderating sales estimates {-}

You are the sales forecaster for a department store chain.
All stores are similar in size and merchandise offered,
but random factors affect sales in any year. Overall sales
are expected to increase by 10% from 2020 to 2021.  Sales
in 2020, with the expected total and mean for 2021 are,
in millions of dollars:
    
Store | 2020 | 2021
----- | ---- | ----
1 | 10 | ---
2 | 23 | ---
3 | 18 | ---
4 | 29 | ---
TOTAL | 80 | 88
MEAN  | 20 | 22

Assuming that year to year correlation is around 0.4,
what is a reasonable estimate for sales in each of the
stores in 2021.  The mean sales amount in 2021 is predicted 
to be 22,000,000 dollars?  With a correlation of 0.4, the 
predicted sales for the individual stores are obtained thus:

Store | 2020 | Subtract 20 | Xply by 0.4, add to 22 | Predicted sales
----- | ---- | ---- | ---- | ----
1 | 10 | -10 | 22-4 | 18
2 | 23 | +3  | 22+1.2 | 23.2
3 | 18 | -2  | 22-0.8 | 21.2
4 | 29 | +9  | 29+3.6 | 32.6
MEAN  | 20 | 0 | 22   | 22

In the real world of 2020 and 2021, the Covid-19 pandemic makes all such
predictions hazardous!  

### Choosing from job applicants {-}

Correlation between presentation & performance is likely to 
be lower for the less well-known.  In both cases performance 
is likely, relative to presentation, to move in closer to the
mean. For less well-known candidates, the shift towards the
mean is likely to be greater.

### Kahneman's comments on regression to the mean {-}

> "Extreme predictions and a willingness to predict rare events from
weak evidence are both manifestations of System 1. ..."  

> "Regression to the mean is also a problem for System 2. The very idea
... is alien and difficult to communicate and comprehend.  This is a
case where System 2 requires special training."  

> "We intuitively want to match predictions to the evidence."  

> "We will not learn to understand regression from experience."

### Regression to the mean in verse {-}
<https://www.youtube.com/watch?v=sxMllckUWaw>

