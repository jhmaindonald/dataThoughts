Systems of human judgment An understanding of the mental traps to which we are prone can help us avoid them. Kahnemans book (2013) Thinking Fast and Slow is a good starting point for thinking about the strengths and limitations of human thinking processes. There is no good substitute for the use of educating gossip as Kahneman describes it for training in effective judgment and in decision making. Important themes that Kahneman notes are We have an excessive confidence in what we think we know. We too readily judge decisions by outcome rather than by the strength of the arguments that support them. We have two selves an experiencing self and a remembering self These do not always have the same interests. Automatic memory formation has its own rules This is exploited to improve the memory of a bad episode We easily think associatively metaphorically casually but statistics requires thinking about many things at once  System  and System  further comments Humans have been conditioned to respond quickly to immediate risks and challenges without stopping to consider too carefully whether what we heard was a false alarm. They also have the ability when the occasion seems to demand it to stop to ponder. This is the basis for Kahnemans categorization of human 
thought processes as of two types System  which jumps rapidly to make a judgment and System  which takes time for careful consideration.1 System  Features are It may answer an easier question in place of a harder. It responds to irrelevancies priming framing affect memory illusions illusions of truth Priming by one stimulus can affect the response to a second stimulus that occurs shortly aferwards The portrayal of logically equivalent alternatives in different ways or frames can affect the response The emotion or affect generated by a question can affect the re sponse It has little understanding of logic and statistics It cannot be turned off but it can be trained When flummoxed it calls on System  System  features are It keeps you polite when angry alert when driving in a severe rainstorm In its world gorillas do not cross basketball courts  Problems that put   in conflict may require large mental effort selfcontrol to overcome the impulses and intuitions of System 1. Its effectiveness depends in important areas on training. Both systems are amenable to training. A welltrained System  helps greatly in creating a better System 1. Further points are Healthy living is a compromise Recognize situations where mistakes are likely Aim to avoid significant mistakes when the stakes are high Untrained humans are poor intuitive statisticians Judgments about statistical issues may require us to think about more than one even many things at once. Too often we jump to conclusions without careful assessment. We may not be equipped to make an informed and carefully thought through decision. 1See httpssuebehaviouraldesign.comkahnemanfastslowthinking for further comment. 2See httptheinvisiblegorilla.comgorillaexperiment.html
 The Intuition of Professionals Effective professional training is designed to ensure that at least some of the results of welltuned System  expert judgment operate at a System  level. The professional will if the training is doing its job build up a repetoire of System  judgments that will later when the circumstances seem to demand it be available at a System  level. The situation has provided a cue this cue has given the expert access to information stored in memory and the information provides the answer. Intuition is nothing more and nothing less than recognition. (Simon  What is an Explanation of Behavior) Obstacles to effective judgment Even those who are experts in their field can be similarly prone to judgments that have no foundation in fact. The following comment appeared in a discussion of the response to a U.S. Preventive Services assessment that prostate screening when used in accordance with then current treatment practices was doing more harm than good.3 Even faced with evidence from a tenyear study of around  men that showed the test didnt save lives many activists and medical professionals are clamoring for men to continue receiv ing their annual PSA test. New evidence emerges as time proceeds and there are advances in the approach to treatment. At least part of the problem has been a rush to treatments that themselves risk increasing damage and the risk of death. Note the comment in Brawley (2018) that Over the past few years the benefittoharm ratio has improved in favor of benefit if the man understands that active surveillance may be a reasonable path if diagnosed.  A demand for discipline careful thought We make judgments based on evidence that is too limited We are easily fooled by irrelevancies 3Association of Professional Psychologists web post on Arkes and Gaissmaier (2012)
Kahneman has brought together evidence on what how. Even when data are there for the taking someone has to notice to collate the data and to understand its uses and limitations Randomized controlled experiments (RCTs) are often the ideal but re quire meticulous planning. If effects of interest are small the numbers required may be very large. See further Subsection  A limitation is that results apply only to the population from which trial participants were taken. Any wider generalization has to be justified Observational data does not easily if at all substitute for the use of RCTs. It is in general impossible to be sure that all sources of bias have been properly accounted for Keep in mind YuleSimpson paradox which we will encounter later in Subsection  The paradox lies in the failure of human intuition to accommodate straightforward arithmetic  Further examples The conjunction fallacy This has also as a result of the example given in Tversky and Kahneman (1983) come to be known as the Linda problem. The name Linda comes from the question and usual response that are given by way of example. Linda is a 31year old philosophy graduate single outspoken and bright. As a student she was deeply concerned with issues of dis crimination and social justice and also participated in antinuclear demonstrations. Which of the following is more probable Linda is a bank teller. Linda is a bank teller and active in the feminist movement. Adding the further descriptor active in the feminist movement can only lower the probability or just possibly leave it unchanged. Instead of assessing the balance of probabilities we are tempted to ask which description best meshes with what we have been already told about Linda. Linda is active in the feminist movement is the single descriptor that respondents see as best fitting Linda. While that was not what was asked one has to pay close attention to prevent System  from substituting that for the question
that was asked. Note that the correct answer will be a bank teller irrespective of the way that Linda was characterized before the question was asked. In part the issue is one of use of language. The correct answer is asking us to use the word probable in a strict technical sense. Even careful critics sometimes get it wrong An irony is that Kahneman was as he has acknowledged himself fooled into taking at face values papers that claimed to show that verbal concepts could have the effect of altering behaviour. Thus Being asked to write down stories about unethical deeds made people more likely to want to buy soap Subtly drawing attention to money e.g. leave banknotes lying around made people feel more selfsuﬀicient and care less about others Priming people with old age related words leads people to walk more slowly away from the lab as research assistants armed with stopwatches timed their movements. As Ritchie (2020) notes (p.28) Kahneman was not alone in being fooled the study about priming with old age related words has been extensively cited in psychology textbooks. None of these claims have stood up in attempts at replication with larger numbers and with greater care to avoid unconscious sources of bias. Thus in the replication of the study relating to agerelated words infrared beams were used to measure time taken to walk between two points in a hallway rather than research assistants who knew the group to which participants had been assigned. Think again a very simple example Is symptom X associated with disease A Symptom X Present Has Disease No Disease Symptom X Absent     The symptom occurs with the same relative frequency whether or not a person has the disease. Nisbett comments that most people including nurses and doctors interpret such evidence wrongly (Nisbett  12930).
A test to check understanding of risk See the 2minute testm Do you understand risk.4  Misbehaving humans The discipline of behavioural economics largely took shape as a result of the work of Richard Thaler. Kahneman was one of two mentors who strongly influenced Thaler the other was Amos Tversky who had worked closely with Kahneman. Thaler and Ganser (2015) explores the extent to which humans do not behave like the rational agents of classical economics agents to whom Thaler gives the name econs. Added to the irrationality with which we often act is that our personal priorities are unlikely to align precisely with those of econs. Note also comments in Part  of Kahneman (2013) titled A conversation with the discipline of economics. In a discussion on The Prospect theory model of choice Kahneman comments on unfortunate tendency to treat problems in isolation Framing effects inconsequential features shape choices Hence a challenge to the assumptions of standard economics.  Negotiating Life in an Uncertain World5 Questions that it can be helpful to ask include 1. Risk of what (Showing a symptom Death) 2. What is the time frame (next  years or lifetime) 3. How big is the risk (Look at risk in absolute terms) 4. Does the risk apply to me (Age sex health ) 5. What are the harms of finding out (False alarms invasive diagnostic procedures unnecessary or dangerous treatments.) 4httpwww.riskliteracy.org 5These questions came originally from the Harding Center web site httpswww.hardingc enter.deen.
Effective use of graphs  General principles Focus the eye on features that are important Avoid distracting features Lines that are intended to attract attention can be thickened Where points should be the focus make them large dark It often makes sense to deemphasize the axes. If points are numerous and there is substantial overlap use open symbols andor use symbols that have some degree of transparency. Different choices of color palettes are effective for different purposes. Bear in mind that the eye has diﬀiculty in focusing simultaneously on widely separated colors that are close together on the same graph.  Varying time intervals show rates not counts A graph that was essentially the solid segmented solid line in Figure  appeared in National Science Foundation (1975) Science Indicators 1974. The segmented line gives a highly misleading impression for the four years  as opposed to earlier points where numbers are totals over decades. It joins up a final point that is a different measure from earlier points. The gray dots and the axis on the right show rates per year thus comparing like with like. 
Total prizes (black line) Nobel prizes per year (gray dots) Number of Nobel prizes      Nobel prizes per year                  Figure  The black line shows numbers of US Nobel prizes for given time intervals. The gray dots. with the right axis label show average per year. The same principle applies for intervals of measures other than time for example of length or volume.  Banking the importance of aspect ratio A Pattern is hidden to view       B Pattern is now obvious             Figure  The same data are used for both graphs. The pattern that is not obvious in Panel A is very obvious in Panel B
Patterns of change on the horizontal scale that it is important to identify should bank at an angle of roughly 45o above or below the horizontal. Angles in the approximate range 20o to 70o may be satisfactory and the aspect ratio should be chosen accordingly.  Scales that show changes by equal multipliers Figure  shows two plots of the same data. Panel A plots brain weight (grams) against body weight (kilograms) for  animals. Panel B plots the same data but now equal distances on each scale show changes by the same factor (i.e. change in relative weight). A Scales show weights B Relative weight scales  African elephant Asian elephant   Human  Brain weight (g)  Brachiosaurus   Triceratops  Dipliodocus         Mouse Body weight (k)       1e05 Body weight (kg) Figure  Panel A plots brain weight (grams) against body weight (kilograms) for  animals. Panel B plots the same data with relative weight scales i.e. equal distances on each scale show changes by the same multiplier. Often when measurement data span a large range (e.g. a change from smallest to largest by a factor of  to  or more) it is a relative amount scale that is appropriate.1 1Technically such scales are termed logarithmic as opposed to straight line or linear. A logarithmic transformation is used to obtain such relative distance scales.
units of  meters or kilometers. Panel B indicates that the pattern of increase moves down to a local minimum at around  meters up to a local maximum at around  kilometer down again to a local minimum at around  kilometers and then steadily up again. Relative distance scales Figure  shows different equal physical distance along the scale labels that might be used for the relative Distance (logarithmic) scale in Figure  in Subsection 2.5.                              Figure  Different labelings all with tick marks at the same relative distance apart are shown for the Distance scale. The multipliers for the Distance values that are plotted are starting at the bottom    and 10.  Helpful web links are Good bad graphs (Ihaka lecture notes)2 Misleading graphs3 Color Brewer4 2httpswww.stat.auckland.ac.nzihaka120Lectureslecture03.pdf 3httpswww.statisticshowto.commisleadinggraphs 4httpscolorbrewer2.org
 Different graphs serve different purposes The line in Figure 2.4A shows the broad overall pattern while Figure 2.4B shows how that pattern needs to be tweaked to more closely reflect the data. A Time versus Distance B Residuals versus Distance 200m 2km 20km 200m 2km 20km    Time (min) Deviations from ﬁtted line30                      Distance (km) Distance (km) Figure  Panel A plots world record Time (as of  in minutes) against Distance (in kilometers) for field races. On both the x and y axes the scale is one on which equal distances on the scale correspond to equal relative changes. Panel B plots deviations from the fitted line in the y direction otherwise known as residuals against Distance. The deviations are approximate relative differences from the line. Thus a  difference is a difference that amounts to  of the time predicted by the line. Notice in Panel A the use of scales for which which equal distances on the scale correspond to equal relative changes. This is achieved by specifying logarithmic scales on both axes. There is a loglinear i.e. straight line on logarithmic scales relationship. In Figure  the line looks to be a good fit. The range of times is however large from just under  seconds to close to  hours. All except the largest difference from the fitted line are a less than  change and are not at all obvious in Panel A. There is a very clear pattern of systematic differences in Panel B that reflects differences in human physiology very likely between the athletes who excel at the different distances. The line can be interpreting as implying a  increase in the time per unit distance for every unit increase in the distance. The units may for example be

Selection and survivor bias In mind here are cases where the data are not a random sample.  The hazards of convenience samples Quota sampling has often been used as an alternative to random sampling quotas are set for age categories malefemale and socioeconomic categories that are designed to ensure that the sample is representative of the wider population. In polls prior to the  US presidential election that pitted democrat Harry Truman against republican Thomas Dewey pollsters were given strict quotas but otherwise left free to decide who they would approach. Polls by three different organizations gave Dewey a lead of between  and 15. In the event Truman led by 5. Convenience samples sometimes have a story to tell This is not to rule out all use of convenience samples. Convenience samples taken within a limited population can sometimes be useful in setting a bound. It is strongly in the public interest that scientists have reasonable freedom for responsible expression of their minds on issues of public concern. In an informal  survey  Crown Research Institute scientists (out of  who responded) answered yes to the question Have you ever been prevented from making a public comment on a controversial issue by your managements policy or by fear of losing research funding The  who responded will undoubtedly be a biased sample. Irrespective of the size of the bias the number who had not 
been allowed to speak their mind was large enough to be a cause for serious concern. Hon Joyces response to the effect that as this was not a scientific survey of all CRI scientists (to this extent true) its evidence of large concern could be ignored was an evasion. Equally disturbing was the reaction of the NIWA management suggesting that they did not accept a responsibility to defend transparency.1  UK cotton worker wages in the 1880s Prior to the Boot and Maindonald (2008) paper2 the main source of published information on cotton worker wages in the UK in the late 19th century were results from an  US Bureau of Labor survey intended for use for comparison with the US cotton industry wages. The line shows what  Spinner Number in  survey  Avwage survey numbers would be   if relative numbers in   worker categories were   as in the  census  Weaver Others  Foreman Labourer Grinders Warehouseman  Big piecer    Number in  census Figure  Cotton worker wages in the UK  US Bureau of Labor survey versus  census data. Wages are in pence per week. Figure  compares the US Bureau of Labor survey numbers with  census numbers of different types of full time UK cotton operatives. The  survey 1See httpssciblogs.co.nzinfectiousthoughts20150828niwainastonishingattackonscientistassociation 2New estimates of ageand sexspecific earnings and the malefemale earnings gap in the British cotton industry 
shows some strong biases a result it would seem of geographical bias and of the informal data collection methods that were used. The high wages given to spinners were grossly overweighted in the US Bureau of Labor survey while Big Piecers and Weavers were grossly underrepresented. A guess is that workers were asked for information on their wages as they left work and that the survey personnel happened to catch employees at a time when there was a large preponderance of highly paid spinners and an untypically small number of big piecers and weavers. The net effect was a gross overestimate of average wages.  The uneasy path from hindsight to insight There is a mix of selection and survivor bias when data from the past are used as a guide to the future with no allowance for the sourcetarget difference. The target about which we wish to make judgments lies in the future while the data are from the past. Think about a business that is planning for the future. One can never know until after the event all the ramifications of the choices made. Businesses may be selected as examples of effective business practice because they were at the time when the data were collated successful. Likewise it is athletes who have been successful in the recent past who are likely to be selected to appear on the covers of sports magazines. In either case this gives a biased picture of what can be expected in the future in many cases those who are picked out will be close to the peak of their success andor have had unusual luck. High levels of success in the recent past will not always translate into success in the following year or years. How often will past success translate into future success In order to discover it is necessary to collect relevant data. Tales of standout past success Collins (2001) in the book Good to Great identified  companies from  studied as standouts. Since   have performed better than average and  worse. Notable later changes in fortune were Fannie Mae   per share   Circuit City Bankrupt in  Waterman and Peters (1982) in the book Close to consumer identified  successful companies as having a bias for action and being close to consumer. From the  that were publicly listed Smith (2014) noted that  had done better than average and  worse. See Smith (2014) for further commentary.
In all cases companies that were chosen as examples of standout success were likely to be near to the peak of their performance as judged by Collins or by Waterman and Peters. Overlaid on this is the regression effect that will be discussed in Chapter 8.  The message in the missing bullet holes In World War II the US air force was concerned that too many of their planes were being shot down. What were the priority areas to which protective armour should be added given that the extra weight meant that they could not be placed everywhere Abraham Walds insight was that survivor bias was to be expected with the density of bullet holes providing evidence about the extent of bias and the implications for identifying the part(s) of the planes that would benefit most from additional protection. See Abraham Wald and the Missing Bullet Holes3 which is an excerpt from Ellenberg (2015). The numbers of hits per square foot were Engines Fusilage Fuel system Rest of plane     Wald argued that the gunshots were likely to have been spread very nearly uniformly over the planes as a whole those that were shot down and those that survived The reason for relatively fewer bullet holes in the engine and fuel system areas was that hits in those areas were more likely to bring the plane down so that they did not return.  httpsmedium.compenguinpressanexcerptfromhownottobewrongbyjordanellenberg664e708cfc3d
Medicine and health A focus on medicine and public health is used as a context in which to introduce ideas and issues that are more generally relevant. There is wide acceptance that the evidence provided by randomized controlled trials (RCTs) that are conducted to high standards is at the top of a hierarchy of evidence. Web sources are noted that because they base their advice on careful and transparent evaluations of the available evidence can be trusted.  Useful sources of advice and information Here are noted web resources that for many issues of major interest provide carefully collated assessments that are based on a critical examination of the whole range of evidence that the authors could identify. There may be many published studies not all of equal quality that bear on a medical issue. Or there may be one or a small number of largescale high quality trials that are used as a basis for judgment. It is important that assessments are updated as new evidence emerges. Resources that will be noted are Harding Center for Risk Literacy The Cochrane Center Winton Centre for Risk and Evidence Communication Note also the US Consumer health information site 
The Harding Center for Risk Literacy There is extensive informative content on the Harding Center for Risk Literacy site.1 Note in particular Medical Fact Boxes. Look under Transfer and Impact Fact Boxes Covid19. Look under Transfer and Impact Corona pandemic Consumer Empowerment. Transfer and Impact Consumer Em powerment Risk and Evidence Communication. Look under Research Risk and Evidence Communication VisRisk Visualization and Communication of Complex Evidence in Risk Assessment. Look under Research VisRisk Horizon2020 Project FORECEE (on balancing female cancer risk against the risk arising from falsepositive alarms and overdiagnosis). Look under Research FORECEE Project Drone Risks Project. Look under Research Drone Risks Project Publications. Look under The Harding Center Publications In the Media. Look under The Harding Center In the Media Harding Center Medical Fact Boxes Medical fact boxes2 provide visual and tabular summaries of the current best evidence from randomized controlled trials. The comparison may be with a placebo or with an alternative that is known to be effective. Detailed references are given. Where available reliance is on Cochrane studies. Available fact boxes as of March  appear under the headings Vaccines Low back pain Antibiotics Early detection of cancer breast colon prostate ovarian Cardiovascular Diseases Osteoarthritis of the knee Tonsil surgery Pregnancy and childbirth Scroll down the home page3 to find the headings Risk Literate (with a link 1httpswww.hardingcenter.deen 2httpswww.hardingcenter.deentransferandimpactfactboxes 3httpswww.hardingcenter.deen
to a risk quiz) and Quick Risk Test (with a link to a test that is targeted at medical students and medical professionals. On the development of this latter test see Cokely et al. (2012). The Cochrane Center4 The Cochrane Centers mission is to promote evidenceinformed health decisionmaking by producing highquality relevant accessible systematic reviews and other synthesized research evidence. They rely heavily on metaanalyses looking for the balance of evidence across all relevant studies.  Randomized Controlled Trials vs other study types Two types of study are widely used in medical and other contexts randomized controlled trials (RCTs) and populationbased studies. These can in both cases be broken down into further subtypes. There may be elements of both these types of studies. Randomized Controlled Trials the gold standard For simplicity it will be assumed that there are two alternatives to be compared in what is known as a twoarmed trial. In medical trials the two arms may be a treatment and a placebo with the placebo (something harmless that has no effect) made to look as similar as possible to the treatment. An important distinction is between preclinical trials often with animals such as mice and clinical trials with human subjects. Preclinical trials are likely to be conducted with a small number of mice or other animals and are intended to check whether a drug or other treatment warrants further investigation. Evidence will be presented in Section  that suggests that these trials are commonly not achieving their intended purpose. Clinical trials typically are conducted in three phases. Phase I applies the treatment to a small number of subjects and checks that the treatment appears safe. Phase II perhaps with several hundred subjects looks for evidence of an effect and how this might relate to dose level. Phase III trials are conducted with large numbers of subjects typically spread over multiple treatment centres and are designed to check whether the treatment is effective what side effects there may be and whether there are issues with particular subgroups. 4httpswww.cochrane.org
Important issues are Use a random mechanism to assign to assign to treatment as against con trol in a medical screening study to screen or not screen The aim is to ensure that apples are compared with apples Treatment and control must otherwise be treated in the same way. There must be strict adherence to a protocol Minor departures that may e.g. allow unconscious bias in the way that results from the different groups of participants are measured can invalidate results. In clinical trials an ideal not always possible is the double blind trial where neither the individual nor the clinician involved knows which drug (or other treatment) the individual has received. Results apply strictly to those who meet the trial entry criteria This may limit relevance to the general population Especially in medical trials think carefully about the outcome measure. It is not enough to show that a screening program will pick up otherwise undiagnosed cancers. In a screening trial e.g. for prostate cancer there are risks both for those who test positive and for those who test negative. The process used to check for cancer may itself bring a smaller or larger element of risk. Positives may be false positives leading to more invasive checks which may themselves carry a risk. Thus for prostate cancer a positive PSA test is likely to lead to a biopsy that itself has been es timated to carry a  risk of serious side effects with a much higher proportion of less serious effects (Levitin  245) Some slow growing cancers may be better left untreated rather than exposing the patient to a treatment that may itself do serious damage. For a helpful animated summary of some of the key issues see httpswww.youtube.comwatchvWy7qpJeozec. Pashayan et al. (2020) provide an overview of progress towards the personalized early detection and prevention of breast cancer noting priority areas for action. A note in passing HiPPO decisions vs AB testing Randomized studies are widely used outside of medicine. Randomization is a key component of the way that Google and others test out e.g. the effect of
different web page layouts. HiPPO Highest paid person in the Oﬀice. The term AB testing is sometimes used to refer to randomized testing of alternatives. AB testing helped propel Obama into oﬀice An experiment was conducted that involved  million people or about  from its email list. The signup forms had one of nine different combinations of images with words on which recipients were invited to click thus Obama photo Learn more Join us Sign up now BW photo of Obama family    Obama speaking       The black and white photo of the Obama family with the words Learn more generated the most clicks. Young (2014) gives an account of AB testing as it might be used for improving library user experience. Population studies groups must be broadly comparable Adjust prunes to look like apples (is it possible) Can one ever be sure that the adjustments do the job Potential for biases is greater than for randomized controlled trials. Where a treatment is compared with a control group the idea is to use a regression type approach to adjust for differences in such variables or factors as age sex socioeconomic status and comorbidities. Propensity score approaches try to summarize such group differences in a single variable (or in principle two or more variables) that measure the propensity to belong to the treatment as opposed to the control group. While their effectiveness for this purpose may be doubted they can be used to provide insightful graphs that check the extent to which the groups are broadly comparable on the variables andor factors used to adjust for differences. The generally negative view of observational studies that is presented in Soni et al. (2019) (studies in oncology) contrasts with the more positive view offered in Anglemyer Horvath and Bero (2014) (health care) for observational studies that have been conducted with high methodological rigour. The strongest evi
dence comes as with the link between smoking and lung cancer from multiple studies with different likely biases that all point strongly in the same direction. Issues for all types of study What are the relevant outcome measures e.g. cancer malignancies found removed or deaths deaths from cancer or from all causes (for some individuals the treatment may be more damaging in it medium to long term effect than the cancer) Care is required to deal with survivor as well as other biases. 4.2.1 False Positives In contexts where the number of false positives is likely to be high relative to the number of true positives screening programs may have serious downsides that outweigh the benefits. Excess iron syndrome known as haemochromatosis affects around  in  in the New Zealand population. Consider a test that has an  accuracy both for detecting the syndrome among those who have it and for not detecting among those who do not.5 Excess iron syndrome known as haemochromatosis affects around  in  in the New Zealand population. Consider a test that has an  accuracy both for detecting the syndrome among those who have it and for not detecting among those who do not.6 Among  tested (10 with and  without) there will on average be  out of  true positives the  without the syndrome will split up as detected ve to detected ve in an  ratio. Thus there will be on average    false positives. Overall those detected as positive split up in a truefalse ratio of  i.e.  or just under  of the positives are false positives. If all positives were detected as positive the  would change to 10406. 5These are known as the sensitivity and the specificity. 6These are known as the sensitivity (true positive rate) and the specificity (true negative rate).
A test with this kind of accuracy becomes much more useful in a subset of the population already known to be at high risk perhaps as identified by a genetic test or perhaps because of a medical condition commonly associated with the syndrome.  Hierarchies of evidence There is broad agreement among medical researchers on the hierarchy of evidence that is set down in the (US Preventive Services Task Force 1989) guide Level I Evidence obtained from at least one properly designed randomized controlled trial. Level II1 Evidence obtained from welldesigned controlled trials without randomization. Level II2 Evidence obtained from welldesigned cohort studies or case control studies preferably from more than one centre or research group. Level II3 Evidence obtained from multiple time series designs with or without the intervention. Dramatic results in uncontrolled trials might also be regarded as this type of evidence. Level III Opinions of respected authorities based on clinical experience descriptive studies or reports of expert committees. The CONSORT  statement (Schulz Altman and Moher 2010) sets out detailed criteria for assessing randomized controlled trials (RCTs). For Level II studies the STROBE guidelines (Erik von Elm et al. 2007) set out reporting standards. Use of such criteria is essential when evidence that is available from multiple randomized controlled trials perhaps supplemented by evidence from studies at lower levels of the hierarchy is brought together in a systematic review. Evidence at level II and especially at level II3 should ideally be checked by conducting an RCT. This is not always possible for ethical or practical reasons. Evidence from one type of study may complement evidence from another. A paper entitled Resolved there is suﬀicient scientific evidence that decreasing sugarsweetened beverage consumption will reduce the prevalence of obesity and obesityrelated diseases (Hu 2013) provides an example. Hu brings evidence from shortterm randomized controlled trials together with evidence from long term cohort studies (4 or  years) to make a convincing case. Clinical trials have their own problems and issues. Using evidence from pub
lished review sources Chalmers and Glasziou (2009) found issues with the choice of research questions the quality of research design and methods and the adequacy of publication practices. They reported that  of studies were designed without reference to systematic reviews of existing evidence and that  were never published in full. Planning and execution failures are set in stone by the time that a research report is sent for review. Preregistration involving the depositing a research question and study design with a registration service or journal before starting an investigation allows peer review feedback that can elicit suggestions for improvement and detect any potential flaws before the study begins.7 Even more than for industrial quality control processes are needed that prevent defects from appearing in the first place with screening for defects at the end of the production line used as a check that those processes have done the job asked of them. Chapter  will discuss issues for using observational data as a basis for inferences.  Avoid or expose infants to peanuts Clinical practice guidelines introduced in or around the year  had recommended the exclusion of allergenic foods from the diets of infants at high risk for allergy and from the diets of their mothers during pregnancy and lactation. It was then a surprise to find that the prevalence of peanut allergy has substantially increased in the recent past doubling in Europe between  and  suggesting that advice given to parents of young children to avoid foods containing peanuts may have been counterproductive. This reassessment was supported at least for infants who at four months had either severe eczema or food allergy or both and thus were at high risk of developing a peanut allergy by the LEAPS study reported in Du Toit et al. (2015). As noted the LEAPS study was limited to infants who at four months had either severe eczema or food allergy or both. Infants were stratified into two groups following a skinprick test with each group then randomized between those exposed to peanut extract and those not exposed. Among  infants in the population who initially had negative results on the skinprick test the prevalence of peanut allergy at  months of age was  7See httpsplos.orgopensciencepreregistration
(37270) in the avoidance group and  (5272) in the consumption group.8 Among the  participants who initially had positive test results the prevalence of peanut allergy was  (1851) in the avoidance group and  (547) in the consumption group. There was no betweengroup difference of consequence in the incidence of serious adverse events. In both groups numbers and percentages are for those who were assigned to the group and whose results could be evaluated whether or not they followed the treatment protocol to which they were assigned. In technical terms these are results from an intention to treat analysis. Such an analysis is designed to mirror what can be expected in practice not everyone who starts off in one group will stick to it. It answers questions about what to do with subjects who did not fully follow the treatment to which they were assigned. The results were followed in  by changes to guidelines that recommended introduction of peanut and other allergenic foods before  months. The assumption that avoiding early exposure to peanuts would reduce risk of later development of peanut allergy was it was judged likely wrong for all infants.  The effectiveness of surgery RCTs are challenging The blurb on the back cover of Harris (2016) states that For many complaints and conditions the benefits from surgery are lower and the risks higher than you or your surgeon think. Humans are very prone to the post hoc ergo propter hoc fallacy it followed therefore it was because of fallacy. Harris argues that unless the benefits of a surgical procedure are clear the only ethical way forward is to do a randomized trial where the procedure is compared with a sham procedure. Such trials are not easy to design and execute. Nonetheless there are a number of important cases where such comparisons have been made. Bloodletting is a prime example of a surgical procedure that has faded away due to evidence not just of a lack of effectiveness but of serious harm.9 The practice attracted widespread debate in the 19th century and into the early 20th century with its defenders making such claims as 8There were twelve further infants in this group whose results could not be evaluated. 9For the history see for example Seigworth (1980)
bloodletting is a remedy which when judiciously employed it is hardly possible to estimate too highly A comment in Watkins (2000) is apt Medical evidence is trusted and we must retain that situation and ensure that it is not abused. It is possible to be an extremely good doctor without being numerate and not every eminent clinician is best placed to give epidemiological evidence. Doctors should not use techniques before they have acquainted themselves with the princi ples underlying them. What are the implications for medical practice  Screening for cancer how relevant is historical evi dence Screening for cancer is an area where if the interest is in risk of death it is necessary to wait for perhaps several decades before there is a high enough number of deaths that results can be usefully evaluated. Changes that can be expected in the interim include Screening may become more sensitive perhaps picking up a higher proportion of relatively benign cancers that are unlikely to ever have serious effects. There may be an increased ability to distinguish between benign and more aggressive cancers. More effective andor less invasive treatments may become available and earlier treatments finessed. All cause death rates are a more relevant measure than cancer specific death rates as treatments may themselves have harmful effects. Whether or not results from clinical trials in past decades remain relevant to current circumstances their results do highlight important questions. PSA Screening for Prostate Cancer more Numbers (rounded) in the following table are from a Harding Center fact box. They are for men  years or older who either did or did not participate in
prostate cancer screening using the PSA test for  years.10 Deaths (prostate cancer)  men No screening  men Screening Biopsy false alarm   Unnecessary treatment     About  out of every  men with screening and  out of every  men without screening died from prostate cancer within  years. This means that  out of every  people could be saved from death from prostate cancer by early detection using PSA testing. Deaths from any cause were around  in both groups. Numbers for benefits are based on four studies with about  participants (progressive cancer) four studies with about  participants (overall mortality) and eleven studies with about  participants (prostate cancer specific mortality). The numbers for harms are based on seven studies with approximately  participants (falsepositive results within three to six participations in PSA testing for early detection) and nine studies with approximately  participants (overdiagnosis and overtreatment). See the web site for references to the studies. Unlike the biopsies that may follow a positive PSA test the PSA test has no direct potential to cause physical harm. Harm results from an undue readiness to use the test result as a reason for further potentially harmful testing and treatment. Wait and watch is often the preferred strategy. See Martin et al. (2018) Levitin (2015 ch. 6) Fung (2020 27881) the web page How Patients Think and How They Should11 and regularly updated summary of the evidence can be found at PDQ Cancer Information Summaries. Breast cancer screening The Raichand et al. (2017) review starts with the comment 10httpswww.hardingcenter.deenearlydetectionofcancerearlydetectionofprostatecancerwithpsatesting  httpswww.nytimes.com20111009booksreviewyourmedicalmindbyjeromegroopmanandpamelahartzbandbookreview.html
The recent controversy about using mammography to screen for breast cancer based on randomized controlled trials over  decades in Western countries has not only eclipsed the paradigm of evidence based medicine but also puts health decisionmakers in countries where breast cancer screening is still being considered in a dilemma to adopt or abandon such a wellestablished screening modality. The short summary last updated in October  from the Harding Center Fact Box for Mammography Screening referring to women  years (a few trials looked at women aged ￿40) and older who either did or did not participate in mammography screening for approximately  years reads Mammography reduced the number of women who died from breast cancer by  out of every  women. It had no effect on the number of women who died from any type of cancer. Among all women taking part in screening some women with nonprogressive cancer were overdiagnosed and received unnecessary treatment.12 The chief English source of evidence for the fact box is the Cochrane review Gøtzsche and Jørgensen (2013). The eight eligible trials included more than  women aged between  and  all reported between  and 1991. One trial was excluded because the randomization had not produced comparable groups. Four trials had inadequate randomization. The three trials with adequate randomization did not find an effect of screening on total cancer mortality. Løberg et al. (2015) provide a slightly more detailed breakdown of the evidence as applied to women who were screened for  years starting at age  with mortality assessed at ages  to  in the UK. Figure  is a visual summary. Interval cancers are cancers that are detected in between regular screens. A prior normal screen may give a false assurance and lead to a delay in seeking help when symptoms appear. See also the regularly updated summary of the evidence atPDQ Cancer Information Summaries. 12httpswww.hardingcenter.deenearlydetectionbreastcancermammographyscreening
Prevented breast  cancer death Prevented  overall death Overdiagnosed  Interval cancer  False positive  with biopsy False positive      Number out of  Figure  Estimates of benefits and harms of screening as applied to the observed incidence of invasive breast cancer (women aged  to  years) and mortality (women aged  to  years) in the UK in 2007. The area by area phasing of the introduction of a mammography screening program in Ireland over  had the character of a natural experiment allowing checks on what the beforeafter difference of each area as it was phased in as against areas where the rollout occurred earlier or later. Moran and Cullinan (2022) looked at data on the tenyear followup of  breast cancer cases. The conclusion was that while invitation to screening increased detection it did not significantly decrease the average risk of dying from breast cancer in the population. The authors did however note that screening may have helped to reduce socioeconomic disparities in late stage breast cancer incidence. The historical data does irrespective of questions regarding its current direct relevance emphasize the importance of tuning breast cancer checks to the risk profile of finding ways to distinguish progressive from nonprogressive cancer and of avoiding overtreatment. Early diagnosis may allow the use of less invasive forms of treatment. As argued in Esserman and WISDOM Study and Athena Investigators (2017) women want better not more screening.

The uses and limits of observational data At least in principle it is relatively straightforward to use regression type methods to make predictions for a set of new data that have been sampled in the same way. What is hard for observational data harder than is commonly acknowledged is to give the model coeﬀicients a causal interpretation. For this it is necessary to have a clear understanding of the processes involved. There will be several perhaps a very large number of explanatory vari ables and an outcome variable. The aim is to find a model that will make predictions for new data. Note the predictivedescriptive distinction. Note the in sampleout of sample distinction. But is the new a random sample of the old population (Is the target a random sample of the source)  We have a prediction. What are the drivers The issues that arise for observational studies do not in general have clear and easy answers. Chapters  and  of Gelman Hill and Vehtari (2020) canvass points that authors of those studies need to address. See also Andrew Gelmans blog.1 There are no simple answers2. All relevant explanatory variables have 1httpsstatmodeling.stat.columbia.edu20181110matching 2See also httpsmathbabe.org20110616thebasicsofquantitativemodeling 
to be identified with the manner in which they may be driving predictions then teased out. Thus in a comparison between two groups (e..g. in Section  midwife led versus medical led neonatal care) one variable or factor may be of particular interest while other variables are used to adjust for differences between the two groups that are at most a secondary focus of interest. Variables that are of secondary interest are commonly referred to as covariates. Regression coeﬀicients can be misleading guides to what is driving predictions if one or more of the relevant covariates is not available or is not properly accounted for. A paradox of the YuleSimpson type sometimes referred to as Lairds paradox has the same potential to mislead. Little that has been published since Rosenbaum (2002) clarifies greatly the advice that can be given for practical data analysis beyond what Rosenbaum has to say. Pearl and Mackenzie (2018) (The Book of Why) offers an interesting assessment. Pearl and his coauthor do a good job of highlighting important issues that should be addressed in order to make causality judgments at the same time overplaying what their methodology can in general achieve. If strictly implemented the standards are so high that they severely limit what they can in practice achieve. Causality diagrams have a central role. There is a detailed and insightful discussion of the history that finally led to the conclusion that smoking causes lung cancer.  Maternal obesity and risk of colorectal cancer Results from a study reported in Murphy et al. (2021) suggest that maternal obesity (￿30 kgm2) did increase the risk of colorectal cancer (CRC) by a factor of 2.51.3 The authors argue that in utero events are important risk factors for CRC and may contribute to increasing incidence rates in younger adults. They are at the same time careful to acknowledge that as an observational study it could not establish cause and that factors such as diet and microbiome might explain the association. The eating habits of mothers must surely have a large effect on what children eat both when young and later in life. To what extent might this explain the association. Obesity is a risk factor for a variety of diseases. Is it obesity that is directly the risk Or is it dietary and other factors that both increase the risk of obesity  error bounds as they are termed run from  to  so that the  risk factor is not very clearly distinguished from .
and of associated diseases  Cholera deaths in London  to   died from cholera in London in 1832. Medical opinion blamed miasma or noxious air associated with the stink from rotting garbage faeces and pollution in the Thames. Poor areas had higher rates of cholera thought to be a result of the more noxious air that arose from crowding and poorer sanitation. Human excreta went into cesspits with nightsoil periodically taken away.4 In  Edwin Chadwick in The Sanitary Conditions of the Labouring Population (1842)5 showed a direct link between poor living conditions disease and life expectancy. Under the assumption that miasma from the cesspools and raw sewage pits was the source of infection the  Nuisances Removal and Diseases Prevention Act6 was passed that led to the dumping of the raw sewage into the Thames which was Londons main source of drinking water. The  epidemic followed shortly after the cesspits were banned. Hassall (1850) in a careful microbiological study commented a portion of the inhabitants are made to consume a portion of their own excrement and to pay for the privilege. By air or by water the  epidemic Farr who worked as statistician in the UK Registrar Generals oﬀice collected data on deaths from cholera in London in the  epidemic. Farr classified districts into three groups thus according to the source of the water for most of the householders 1) Lower Thames coded as Battersea 2) Sources away from the Thames coded as NewRiver 3) Further up the Thames than Batersea where the water was less polluted coded as Kew. 4See Cholera epidemics in Victorian London httpswww.thegazette.co.ukallnoticescontent100519  httpswww.sciencemuseum.org.ukobjectsandstoriesmedicinecholeravictorianlondon 6Gazette issue 
Figure  summarizes results from a regression analysis that used Farrs data. None of the terms stands out as substantially more important than any other. Higher rates for the poor where crowded conditions would commonly make it diﬀicult to maintain hygiene were to be expected. Partial residual Partial residual    Partial residual          BatterseaNewRiver Kew          elevation poorrate water Figure  Each panel shows in turn the estimated contribution of a term in the model relative to the mean contribution from other model terms. Changes in deaths are on a log scale so that an increase by one unit multiplies the odds of death by close to  around an overall mean of just over six per 1000. Snow (1855a) gave examples that he had observed directly where the likely means of transmission of the infection appeared to be a water source or poor hygiene. He argued that those living close to the Thames and especially in the South were more likely to be getting their water that was contaminated with human excreta. Contaminants had more time to settle in water that was piped up to higher ground. Farr took Snows arguments seriously but in his  report argued that water was primarily important as a vehicle for miasmata. He would later by the time of an  epidemic when Snow was dead be one of the waterborne theorys few champions.7 A context has to be provided in which to interpret regression results such as those shown in Figure 5.1. Snows understanding of the contextual information was not in  suﬀiciently compelling to persuade other medical specialists. Data from the  epidemic which allowed a comparison of deaths supplied from a company that continued to get its supply from lower highly polluted Thames water with that from the company that had moved its supply higher up to less polluted water seems in retrospect to clinch the issue but did not at the time convince most of the medical profession.8 The perspective brought by 7Eyler (1973) 8See Eyler (2004) for further comment.
germ theory had to wait for the work of Pasteur in the late 1850s and Koch in the 1880s. The  epidemic a natural experiment Two water companies Lambeth and Southwark and Vauxhall had been taking water from the same polluted source. An  act required water supply companies to move water intake upriver by 1855. By the time of the  epidemic Lambeth had moved its intake  miles upriver while the Southwark and Vauxhall intake was unchanged until 1855. Data on the distribution of cholera in the  epidemic then allowed Snow to test the claims made in his  study. Southwark Vauxhall Houses Deaths Rate per  Lambeth    Rest of London       The experiment too was on the grandest scale. No fewer than  people from gentlefolks down to the very poor were divided into two groups without their choice and in most cases without their knowledge one group being supplied with water containing the sewage of London and amongst it whatever might have come from the cholera patients the other group having water quite free from such impurity. (Snow 1855b)
Use water from the brewery and stay healthy Snow's Cholera Map with Pump Polygons  m.  Castle St E Oxford Market OxfoOrdxfSotrd1St  Gt Marlborough Crown Chapel Broad St So SohoDean St WarwickBriddle St Vigo St Coventry St Figure  Deaths (red dots) and pump locations. Polygons that surround each pump enclose the locations for which that is the nearest pump. Snow noted that Within  yards of the spot where Cambridge Street joins Broad Street there were upwards of  fatal attacks of cholera in  days. By contrast none of the employees of a local Soho brewery developed cholera. The reason he judged was that they drank water from the brewery (which had a different source from the Broad St pump) or just drank beer alone. Coleman (2019) gives detailed comments on Snows work. New Zealand cities had similar issues from the 1840s and 1850s through until the end of the century arising from failures to install proper drainage systems.9 9See Christine Dann Sewage water and waste Stinking cities Te Ara the Encyclopedia of New Zealand (8 June 2017) httpsteara.govt.nzenzoomify24431dunedinrenamedstinkapool
 Are there missing explanatory factors The (Wernham et al. 2016) study used data from  singleton term deliveries that occurred between  and  to make the claim that midwife led care as opposed to medical led care gave a greater risk of adverse fetal and neonatal outcomes. Notably the claim was that midwife led care resulted in a lower Apgar score (a measure of infant health immediately after birth) and a greater risk of the imprecisely defined diagnosis of birth asphyxia. This study was then the basis for exaggerated claims in an article in the October   issue of the NZ Listener (Chisholm  Birth Control). Contrary to what was claimed the research did not lob a grenade into the historically wartorn territory of New Zealands maternity care. Even less did its results warrant the melodramatic claims of Alarming maternity research and Revolution gone wrong that appeared on the Listeners front cover. A major issue with the analysis is that it relies on using the NZ Deprivation Index10 to adjust for socioeconomic differences. This provides a deprivation score for meshblocks each of around  people. It estimates the relative socioeconomic deprivation of an area and does not directly relate to individuals. Deprived areas will often include some individuals with high socioeconomic status. Caesarean section as a delivery type may well have been more accessible for those of higher socioeconomic status. For National Womens in Auckland the elective Caesarean rate at term over  for doctorled care was  as against  for self employed midwives (Farquhar McCowan and Fleming 2016). Effects from fetal alcohol syndrome were not accounted for nor were direct effects from substance abuse. International data indicates that fetal alcohol syndrome may affect as many as  of births.11 Studies that are similarly relatively carefully done but naive in the weight placed on the regression results are embarrassingly common. There are analysis tools and associated graphs that the authors of the study could and should have used to shed light on the likely effectiveness of the adjustments made for differences between the two groups other than whether the delivery was midwife led or medical led. 10httpswww.health.govt.nzpublicationnzdep2013indexdeprivation  httpswww.health.govt.nzourworkdiseasesandconditionsfetalalcoholspectrum disorder
 The uses and traps of rulebased methods Figure  shows the distributions of values of six variables that have been selected for use for present illustrative purposes from an historical dataset (pre  now long past its use by date) that has data on  email messages of which  were identified as spam. In practical use such datasets have to be continually updated as spammers change their strategies No. 'money' as of words No. of '' as of symbols No. '' as of symbols   No. 'make' as of words No. '000' as of words Total runs of capitals                            ny ny ny ny ny ny Figure  Boxplots showing distribution of variable values in data used to predict email spam Two types of decision tree approaches will be discussed the use of individual decision trees and the random forest approach which generates and uses large numbers of trees in the decision making process. Figure  shows a decision tree that has been derived for the spam data. dollar0.0555 y Symbols used in tree are bang  dollar Number of symbols crl.tot  (as of symbols) n bang Number of symbols bang  (as of symbols) y crl.tot Total runs of capitals crl.tot  n ny Figure  Decision tree for spam data. If the condition is satisfied take the branch to the left. Otherwise take the branch to the right.
The tree in Figure  would be too inaccurate for practical use even suitably updated with new data but it is easy to follow the decsion tree process. From trees to forests Random forests improve on decision trees by using samples from the data to create a forest (a random forest) of trees then voting between the trees. A downside is that Random forests and similar methods operate largely as black boxes. For detection of spam email this may as those who deploy the spam detectors have little idea what may be going on in the minds of the spammers not be too much of an issue. One wants a spam detector that will respond effectively to whatever is thrown at it. It helps to know the how and why of the algorithms used Both decison trees and random forests follow an algorithmic process. The relatively black box nature of the random forest approach places an especially strong burden on the analyst to ensure relevant data have been used and that the algorithm really is doing its intended task. In her book Weapons of math destruction ONeil (2016) comments its not enough to just know how to run a black box algorithm. You actually need to know how and why it works so that when it doesnt work you can adjust. This is too strong. But if one does not know the how and why of how an algorithm works it is absolutely crucial to be sure that the data used to fit and test the model (the training and test data) are directly relevant to the task in hand. Automated systems that can be easily gamed do however abound. They are a menace

Weighting effects that skew statistics  Covid19 deaths comparing countries from Covid19 All causes Deaths per                  Age Figure  US data for proportions who died from Covid19 and in total for the  months up to  January 2021. Vaccination of a substantial part of the population and the emergence of new variants will six months or more later have substantially changed the pattern of relative risks by age. Figure  shows stark differences by age in US Covid19 death rates. The US death rates per  for under age  as opposed to age  were   US totals for infection rates and for hospital admissions were also impacted 
by age structure but to a lesser extent. More recent Covid19 variants would present a somewhat different picture. Countries with a lower proportion of their population aged  would if the death rates for each of the two groups are similar to US rates have lower overall death rates. The following compares the overall deaths rates for the US with what might if these figures carried across be expected for Kenya and for Italy. US Italy Kenya Percentage  or more    Expected deaths per     Reported deaths per     Between country comparisons are hazardous. The dependence of reported case numbers on testing rates and reporting protocols makes it likely that they will be substantial undercounts to an extent that varies from country to country.  University admissions data Simpsons paradox Admissions data for University of California Berkeley in  showed a curious anomaly. Overall admission rates strongly favoured males while in individual departments the rates mostly favoured females. The table shows percent admission rates with number applying shown in brackets underneath. Male OVERALL A B C D E F Female        (2691) (825) (560) (325) (417) (191) (373)       (593) (375) (393)  (1835) (108) (25) (341) Figure  provides a graphical summary.
All females All males   Males Number of applicants Females      C A B FD E      UCB Admission rates ()  Figure  UCB admossion data for  for males and females by department. Department labels range from A to E. Notice that the largest differences in admission rates are for departments A and B in both cases favouring females. There are thus three different broad admission patterns. The big differences in admission rates were in departments A (82.462.120.3) and B (68635) in both cases favouring females. In the other four departments differences were  or less and split very nearly equally between slightly favouring females and slightly favouring males. Also the relative numbers of males and females applying did not show the same big differences as in departments A and B. Table  Comparion of admission rates (percent) and numbers of males and females applying for departments A B and CDEF which combines numbers for departments other than A and B. Male TOTALS A B CDEF Female     (2691) (825) (560) (1306)    (108)  (1702) (1835) (25) The overall male rates are weighted (825560)(325(417191373
 or  between an overall AB rate of  and the CDEF rate of 25.5. Overall female rates are weighted   for departments A and B as against departments CDEF. Overall female rates are accordingly strongly weighted towards the  rate for other departments. UCB Admissions Data Another perspective Female Male difference () MaleFemale ratios of numbers  A       who applied are shown alongside each letter F  D  B  E  C      Order of difference Figure  UCB admossion data for  another perspective. In order to understand how the overall imbalance between numbers of males and females arose it was necessary to break the data down by department. The major driver of the overall imbalance was the low numbers of females relative to males applying to department A (and to a lesser extent B) where admission rates were highest. See httpswww.youtube.comwatchvZDinnCwP3dg for an animated video that explains the YuleSimpson paradox. A note on Lords paradox The same sorts of paradoxical effects can be found in regression. The YuleSimpson paradox may be regarded as a special case of Lords paradox described in Lord (1967).1 Any attempt to attach meaning to regression coeﬀicients can 1See also Tu and Gilthorpe (2011) pp.6071
be highly misleading unless it is clear that effects of all relevant variables are properly accounted for. It is rarely easy with observational data to be sure that this has been done effectively a point that will be taken up in Chapter 8.  Comparing unvaccinated with vaccinated Once high enough vaccination rates have been achieved a greater number of Covid19 infections will be found among the vaccinated than among the vaccinated. Suppose that for a particular age group and for a particular vaccine the risk rate for severe Covid19 is  for unvaccinated versus  for vaccinated i.e. a relative risk of 51. Consider the following scenarios. The final column has the relative numbers for unvaccinated versus vaccinated with Covid19 Unvaccinated Vaccinated Relative numbers with severe Covid19    unvaccinated  among vaccinated         At a certain point provided the risk ratio is greater than  the higher number will be from the vaccinated. Of course the  vaccination rate scenario in the final row is unlikely to be realized. Report found almost  of cases were vaccinated As of August   Israeli data showed that  of those hospitalized with severe Covid19 were fully vaccinated while  were not. We need to compare not the counts but the proportions of vaccinated and unvaccinated i.e. look at the percentages that are shown in brackets following the counts.2 Numbers given are of those aged  or more. Number (aged 12) Unvaccinated Fully vaccinated Severe cases    (16.4)  (5.3) 2Data are from httpsbit.ly3h1mwXQ on the website httpswww.coviddatascience. com.
The relative risk is 16.45.3 3.1. Because no account has been taken of age differences this still underestimates the effectiveness of vaccination. A fairer picture is Number  Unvaccinated Fully vaccinated Severe cases   Number   (3.9)  (0.3) Severe cases    (91.9)  (13.6) The relative risk is 3.90.3  for those under  and 91.913.6  for those over  i.e. in both cases much greater than the  ratio obtained when the split between under 50s and  or more is ignored. Even this does not do justice to the data which needs to be split into smaller age ranges.  Further illustrative examples Does Baclofen help in reducing pain Baclofen No baclofen male   female   ALL         Average reduction  min vs  min Figure  Data are pain reduction scores. Subgroup numbers shown below each point weight the overall average (ALL) for baclofen towards the high female average and for no baclofen slightly towards the low male average.
In work reported in Cohen (1996) researchers were comparing two analgesic treatments without and with baclofen. When the paper was first submitted for publication an alert reviewer spotted that some of the treatment groups contained more women than men and asked whether this might account for the results. For a fair overall comparison Calculate means for each subgroup separately. Overall treatment effect is average of subgroup differences. The effect of baclofen (reduction in pain score from time 0) is then Females    (ve therefore an increase) Males    Average male female  (0.6720.336)  Web page revenue per click Smith (2014) (p.111) describes an experiment where a US Internet company collected data that compared the effectiveness of two strategies. In the 1click strategy an advert appeared on the websites first page. The 2click strategy required the user to click on a keyword which then led to a page with the advert. The response (total dollar value of purchases) was compared between the two. The following with numbers changed to make the comparison relatively simple shows a scenario that is not unlike that given by Smith 1click Users RP1000 2click Users RP1000     Revenue Revenue   The 1click strategy appears to give a better return. Now see how the numbers divide up when a split is made between US and international visitors to the site. The numbers (thousands) for the two strategies divide up thus with the revenue per thousand users given in each case in brackets
1click 2click US   (32) (30)  Int  (5) (6) The 1click strategy clearly gives a better return both for US and for international users. The overall figure is dominated by the result for the  international users (as opposed to  US) in the 1click sample. This compares with the much weaker weighting towards international users (120000 as opposed to 80000) in the 2click sample. In the case that Smith reported the type of analysis shown was followed up with a randomized experiment (an AB test) where the probability of assignment to 1click as opposed to 2click was the same for both classes of user.  Cricket Bowling Averages Runs (R) wickets (W) and runs per wicket (RPW) 1st innings 2nd innings Overall R W RPW R W RPW R W RPW Bowler A          Bowler B          Fair comparison Compare runs per wicket (RPW) 1st innings 2nd innings Overall RPW W RPW W RPW W Bowler A  (4)  (6)   (10) Bowler B  (5)  (1)  (6)     Epistatic effects in genetic studies In population genetics Simpsons paradox type effects are known as epistasis. Most human societies are genetically heterogeneous. In San Francisco any gene
that is different between the European and Chinese populations will be found to be associated with the use of chopsticks If a disease differs in frequency between the European and Chinese populations then a naive analysis will find an association between that disease and any gene that differs in frequency between the European and Chinese populations. Such effects are major issues for genedisease population association studies. It is now common to collect genetic fingerprinting data that should identify major heterogeneity. Providing such differences are accounted for large effects that show up in large studies are likely to be real. Small effects may well be epistatic.

Matters of consequence  The MMR vaccine scandal The MMR vaccine was developed to for use in preventing measles mumps and rubella. Andrew Wakefield was the lead author of a study published in  based on just twelve children that claimed to find indications of a link between the MMR vaccine and autism. The journalist Brian Deer had a key role in identifying issues with the work including fraudulent manipulation of the medical evidence. It emerged that Wakefield had multiple undeclared conflicts of interest Funding came from a group of lawyers who were interested in possible personal injury lawsuits From  children said to have regressive autism Only  had been diagnosed  had no autism  had developmental problems before the vaccine Wakefields  claims were widely reported Vaccination rates in the UK and Ireland dropped sharply The incidence of measles and mumps increased resulting in deaths and in severe and permanent injuries. Wakefield was found guilty by the General Medical Council of serious professional misconduct in May  and was struck off the Medical Register. 
Following the initial claims in  multiple large epidemiological studies failed to find any link between MMR and autism. Fact boxes on the Harding site summarize evidence of the effectiveness of the MMR vaccine1  Sally Clarks disturbing cot death story Sudden Infant Death (SID) also referred to as cot death is the name given to the unexplained sudden death of very young children. The story of Sally Clarks unfortunate brush with the law following the death of a second child is interesting disturbing and educational. Sallys experience highlighted ways in which the UK legal system needed to take on board issues that affect the use of medical evidence issues of a type that can be important in medical research. Paediatrician Sir Roy Meadow had argued in his  book ABC of Child Abuse that unless proved otherwise one cot death is tragic two is suspicious and three is murder. Was this an example of Too hard Try something easier and wrong Or was it the triumph of assumed knowledge over hard evidence Meadows role as an expert witness for the prosecution in several trials played a crucial part in wrongful convictions for murder. The case that attracted greatest attention is that of Sally Clarke both of whose children were cot death victims. Following the death of the second child Meadow gave evidence at her  trial and appeal in 2000. Sally Clark was finally acquitted in 2003. Meadow gave  in  as cot death rate in affluent nonsmoking families Squared  to get odds of  against for two deaths. Meadow assumed wrongly that the probability of a death from nat ural causes was the same in all families. A first cot death is in some families at least evidence of a greater proneness to death from natural causes. Royal Statistical Society press release Figure has no statistical basis  appeal judges The figure was a sideshow that would not have influenced the jurys decision. The appeal judges statement was described by a leading QC not involved in the case as a breathtakingly intellectually dishonest judgment.  It emerged that 2nd death was from bacterial infection 1httpswww.hardingcenter.dedesearchnodekeysmmr
In a second appeal Sally Clark was freed.  Meadow was struck from medical register  reinstated by appeal court misconduct fell short of serious Meadow was in effect assuming without evidence the absence of familyspecific genetic or social factors that make cot deaths more likely in some families than in others. Why was Meadows assumption of independence not challenged in the  trial Issues of whether or not different pieces of evidence are independent are surely crucial to assessing the total weight of evidence. Meadow had only enough understanding of probability to be dangerous. Sally Clark was finally acquitted on her second appeal in  with her sense of wellbeing damaged beyond repair. The forensic evidence had been weak. The web page httpsplus.maths.orgcontentosissue21featuresclarkindex has a helpful summary of the statistical issues. It quotes a study that suggests that the probability of a second cot death in the same family is somewhere between  in  and  in 130. Even after this adjustment the probability of death from natural causes of two children in the same family is low. But so also is the probability that an apparently caring mother from an affluent middle class family with no history of abuse will murder two of her own children. Those are the two probabilities that must be compared. Anyone who plans to work as a criminal lawyer ought to understand this crucial point. In a large population there will from time to time be two deaths from natural causes in the one family. A small number of appeals were subsequently launched against other convictions where evidence of the same type had been presented most of them successful. A further consequence was that the law was changed such that no person could be convicted on the basis of expert testimony alone. The Watkins (2000) article Conviction by mathematical error Doctors and lawyers should get probability theory right provides a trenchant critique of the perils of allowing nonstatisticians to present unsound statistical arguments with no effective challenge. Guidelines for using probability theory in criminal cases are urgently needed. The basic principles are not diﬀicult to understand and judges could be trained to recognise and rule out the kind of misun derstanding that arose in this case. Watkins who was at the time Director of Public Health for Stockport in the UK argued that the calculation of the relevant probability should have had no regard to the probability of an initial cot death. It surely has some relevance.
The basic principles are perhaps more diﬀicult than Watkins was willing to allow.  The Reinhart and Rogoff saga Figure  plots data for   from  advanced countries that underpinned the  paper Growth in Time of Debt by the two Harvard economic historians Reinhart and Rogoff 𝑅𝑅 . GDP growth vs DebtGDP ratio For  ratio cf dashed vs dotted lines GDP growth () AT  NZ  BE  AU Australia  NZ  AT Austria BE Belgium AT  NZ  FI Finland  NZ  IE Ireland IT Italy JP Japan  BE  UK  IT  UK  FI  IE  NZ  UK  AU   NZ  JP     US        Ratio of debt to GDP () Figure  Dashed horizintal lines show means by DebtGDP category for  advanced countries for the years  2009. (Data are missing for some countries in some years.) RRs mean (dots) for  DebtGDP was from  only of the 20. The smooth gray curve treats points as independent. The paper (Reinhart and Rogoff 2010) has been widely quoted in support of economic austerity programs internationally. There was a huge stir in the media
and on the blogosphere when graduate student Herndon found and published details of coding and other errors in the results that RR had presented.2 As well as coding errors Hendon identified selective exclusion of available data and unconventional weighting of summary statistics. There was no consideration that the relationship studied has varied substantially by country and over time. Half of the  countries had missing data for  or more of the years with the largest number missing in the years  to 1949. In response to Herndon Ash and Pollin (2014) and other critics Reinhart and Rogoff accepted that coding errors had led to the omission of several countries but pushed back against other criticisms. Their revised analysis addresses only the most egregious errors in their work. Among other issues their insistence on treating each data point for each country as an independent piece of evidence makes no sense. The smooth curve fitted in Figure  may be regarded as an average over the ten countries but as Figure  shows with huge country to country variation. NZ  GDP growth ()  Greece  Canada  Ireland Belgium Japan  US Australia Italy UK      Ratio of debt to GDP () Figure  Smooths have been fitted for each of the  countries for which debt to GDP ratios were in some years greater than 90. There is no consistent pattern as there should be if RRs claim is to hold up. 2httpwww.peri.umass.edufileadminpdfworkingpapersworkingpapers301350WP322.pdf
Is there a pattern across countries There were just  countries where debt to GDP ratios were greater than  for one or more years. For  of those countries the average of their GDP growth in the years at issue was on average positive relative to the previous year. The average percentage growth over all  countries weighted according to number of years was 2.168. There are further serious issues of interpretation Does GDP drive debtGDP ratio or is it the other way round Or does a third factor drive both Is the effect immediate or on future economic performance Smith (p.64) refers to work indicating that economic performance is more closely correlated with economic growth in the past than with future growth. Parting comments Herndon Ash and Pollin (2014) comment RRs findings have served as an intellectual bulwark in support of austerity politics. The fact that RRs findings are wrong should therefore lead us to reassess the austerity agenda itself in both Europe and the United States. The saga emphasizes the importance of working with reproducible code rather than with spreadsheet calculations. The errors in RRs calculations were from one perspective fortunate. Once highlighted the errors drew critical attention to the paper and to the serious flaws in the analysis.  What do malaria drugs do to Covid19 patients3 Thirteen days after it was published on May   three of the four authors withdrew a paper that claimed to find that malaria drugs when used experimentally with patients with Covid19 led to around  excess deaths. Irrespective of the problems with the data that will be noted shortly serious flaws in the analysis ought to have attracted the attention of referees. There was inadequate adjustment for known and measured confounders (disease severity temporal effects site effects dose used). 3Lancet May  httpsbit.ly3xqncMt
The study claimed to be based on data from  hospitalized COVID19 patients from six continents of which  were from North America. Very soon after it appeared the article attracted critical attention with a number of critics joining together to submit the Watson et al. (2020) letter to Lancet. The sources from which the data had been obtained could not be verified data that claimed to be from just five Australian sources had more cases than the total of Australian government figures and similarly for Australian deaths there were implausibly small reported variances in baseline variables mean daily doses of hydroxychloroquine that were  mg higher than US FDA recommendations. Randomized trials designed to test the effectiveness of the drugs and that were in progress at the time when the paper appeared were temporarily halted. The eventual conclusion was that the drugs did not improve medical outcomes. There was some evidence that hydroxychloroquine could have adverse effects. With current webbased technology randomized controlled trials can be planned and carried out and yield definitive answers in much the same time as it would take to collect and analyze the data that are required for an observational study whose conclusions can be at best suggestive. Data confidentiality issues are easier to handle in the context of an RCT.  A simplistic use of publicly available data A June  paper in Vaccines titled The Safety of COVID19 Vaccinations We Should Rethink the Policy.4 massively overstated vaccine risk. The paper claimed that For three deaths prevented by vaccination we have to accept two inflicted by vaccination. Deaths were from any cause postvaccination reported both by profes sionals and the public (Such data has to be used with a baseline comparison) Vaccine benefits extend far beyond  weeks of Israeli study The paper focused on immediate risk to individual not community. The way that any risk from the vaccine balances out against risk of death from Covid19 will vary depending on the the age structure of the popu lation on proportion immunized within each age group and on the age and health of the individual. 4Vaccines June  bit.ly3dTg1oh
Deaths that could be verified as from the Pfizer vaccine have been with the possible exception of frail elderly people extremely rare. See US CDC report on risk (for Pfizer anaphylaxis)5 and Helen PetousisHarris commentary on risks.6 5www.cdc.govcoronavirus2019ncovvaccinessafetyadverseevents.html 6sciblogs.co.nzdiplomaticimmunity20210415covidvaccinesandbloodclotswhatisthisabout
Regression and Correlation When two variables show a relationship they are said to be correlated. Regression and correlation offer alternative and complementary perspectives on the relationship.1 Nonsense correlations that arise where the third variable is time provide simple examples.2  Correlation is not causation Variable A may cause variable B. Or variable B may cause variable A. Or both A and B may be caused (or driven) by a third variable C. The following are examples where causation likely goes in the other direction or where a third variable is likely to be involved. Such examples help highlight how correlation can and cannot reasonably be interpreted. 1. Children of parents who try to control eating are more likely to be over weight. 2. Ice cream consumption polio were closely correlated in the 1950s. Summer was when the virus thrived. Cases where the third variable is time as in Figure  are a fruitful source 1The discussion will avoid the technicalities of alternative ways to measure correlation. The particular correlation measure that is used in the examples that follow is the productmoment or Pearson correlation which relates directly to linear (straight line) regression. 2YuleSimpson type effects discussed in Section  are important in a regression context also. 
of examples of spurious correlations.3 Is there a third factor or is this just a chance relationship Doctorates Deaths       Doctorates600   Deaths30 Doctorates20550         Deaths      Figure  Sociology PhDs awarded (from US National Science Foundation data) vs Deaths from Anticoagulants. Notice that Doctorates and Deaths show a very similar pattern of change from year to year. Another example is that over  to  US Sociology PhDs awarded correlated strongly with worldwide noncommercial space launches. These correlations would seem to be the result of chance. Look at enough variable pairs and such correlatiosn will sometimes appear.  Regression to the mean Tall fathers are likely to have tall sons but shorter than themselves. Tall sons are likely to have tall fathers but shorter than themselves. The data shown in Figure  are from Pearson and Lee (1903). The correlation between sons height and fathers height is 0.5.4 Notice that the points that are plotted show a a symmetrical elliptical shaped scatter about the mean (shown with a large solid dot in Panel A). This type of scatter is strictly required for uses of the correlation that will now be discussed. 3Figure  is one of many such examples that are available from httpwww.tylervigen.comspuriouscorrelations. 4Kahneman argues perhaps too simplistically that as height is mainly due to genetic factors and fathers share half of their genes with their sons this is to be expected.
A Son's height given father's height B Father's height given son's height 95th percentile 79th percentile r  r  Son's height (in)     80th percentile Son     95th percentile         Father's height (in) Father Figure  Tall fathers are likely to have tall sons but shorter than themselves. Tall sons are likely to have tall fathers but shorter than themselves. Consider a father whose height is  inches which is the 95th percentile of heights for fathers. What is a best guess for the height of a son One can read the predicted value off from the graph (the solid horizontal line in Panel A). Or use the regression equation. Or reason thus If the correlation between fathers height and sons height were  the best guess would be the mean for sons i.e.  inches If the correlation were  the sons height would be the 95th percentile of heights for sons i.e.  inches. But as the correlation is  the expected height is  0.5(73.368.7)  inches i.e. start at the mean and move  of the distance up to the 95th percentile. Now consider a son whose height is  inches (the 95th percentile for sons). The argument now goes The best estimate of the fathers height is  0.5(73.367.7)2.8 inches i.e. start at the mean for fathers and move  of the distance up to the 95th percentile. Galtons  data which predates Pearsons data shows a  correlation between child height and the average of the parent height.
 NBA player points correlations decline over time In Figure  Panel A shows total points for  versus  year earlier for players who competed in both seasons. The correlation is 0.83. Panel B is for  versus  years earlier with the correlation now reducing to 0.41. A  vs  B  vs    Total points   Total points                   Total points  Total points  Figure  As time progresses correlation decreases and regression to the mean increases. For Panel A the correlation is  while in Panel B it is 0.41. The scatter of points increases as values increase on both axes. Calculations of the type given in the previous section based on the usual correlation measure while giving more approximate results are adequate for present purposes.  Secrists The Triumph of Mediocrity in Business Horace Secrists  book The Triumph of Mediocrity in Business was based on annual data for  to 1930. Secrist took  different industries in each case examined the ratios Profitssales Profitsassets Expensessales Expensesassets For each industry in  he then split firms into  quartiles top  2nd highest  2nd lowest  lowest 25. Took average for each statistic for each quartile for each year.
Surprise surprise the best went on average down Complete freedom to enter trade and the continuance of competi tion mean the perpetuation of mediocrity. neither superiority or inferiority will tend to persist. Rather mediocrity tends to become the rule. (Secrist 1933) A Forwards from  B Looking back from  Highest  in  Highest  in  ProﬁtSales     ProﬁtSales     2nd highest  in  2nd highest  in  2nd Lowest  in  2nd Lowest  in  Lowest  in  Lowest  in              Figure  Secrists data showed a correlation of  between time intervals five years apart. Panel A uses shows means of simulations starting with the four performance quartiles in  and looking ahead. Panel B starts with the equivalent quartiles in  and looks back. Secrist was seeing regression to the mean. Figure  makes the point that if one takes the four quartiles in  and looks back to  in each case there is a regression back to the mean. Given a correlation of  between time intervals five years apart The absolute difference from the mean moves from  to   ( 5.6) to   ( 3.92) whether one moves by two successive five year intervals forward in time or back in time. Do old fallacies ever die Smith (2014) gives references to work by prominent economists in the past halfcentury that had quoted Secrist approvingly or repeated his error. 1980s investment textbook Ultimately economic forces will force the convergence of the profitability and growth rates of different firms. This was backed up with a  Secrist type comparison.
 (Journal article) profitability is meanreverting within as well as across industries. Other firms eventually mimic products and technologies that produce above normal profitability Wainer (2000) cites other examples. Decathlon performances in  Polevault Javelin Discus Shotput Highjump Longjump Polevault Javelin              Discus Shotput Highjump Longjump                           1.81.92.02.1      3.03.54.04.55.05.5         Figure  Between event correlations for top performances in six of the ten decathlon events in 2006. Points that are plotted and correlations are for times or distances achieved. Figure  shows between event correlations for top performances (6800 points and over) for six events in the  decathlon.5 Note the  correlation of javelin with high jump. Performance in one of these two sports was not a useful indicator of what to expect in the other. The correlation between shot put distances and long jump distances is shown as a more informative 0.29. If we find that an athlete has achieved a distance of 5The dataset Decathlon in the R package GDAdata has data for the 21year period after new rules were introduced in 1985. See also the Estonian website httpwww.decathlon2000.com
 meters in the shot put which is at the 93𝑡ℎ percentile across athletes as a whole (7 did better) a best estimate for the long jump can be obtained thus A  meter put is at the 93𝑡ℎ centile (7 will do better) The long jump mean is  with the  mark  The difference from the mean is 7.476.97  The estimate for the long jump is then 6.970.29    Moderating predictive assessments Moderating sales estimates You are the sales forecaster for a department store chain. All stores are similar in size and merchandise offered but random factors affect sales in any year. Overall sales are expected to increase by  from  to 2021. Sales in  with the expected total and mean for  are in millions of dollars Store           TOTAL  t each of the stores in 2021. The mean sales amount in  is predicted to be  dollars With a correlation of  the predicted sales for the individual stores are obtained thus Store  Subtract  Xply by  add to  Predicted sales                     MEAN     In the real world of  and  the Covid19 pandemic makes all such predictions hazardous
Choosing from job applicants Correlation between presentation performance is likely to be lower for the less wellknown. In both cases performance is likely relative to presentation to move in closer to the mean. For less wellknown candidates the shift towards the mean is likely to be greater. Kahnemans comments on regression to the mean Extreme predictions and a willingness to predict rare events from weak evidence are both manifestations of System 1. Regression to the mean is also a problem for System 2. The very idea is alien and diﬀicult to communicate and comprehend. This is a case where System  requires special training. We intuitively want to match predictions to the evidence. We will not learn to understand regression from experience. Regression to the mean in verse httpswww.youtube.comwatchvsxMllckUWaw  Time per unit distance for hillraces Regression coeﬀicients may differ greatly depending on what adjustments are made for other variables. This is important for attaching meaning to a coeﬀicient. For the hillrace data that will now be considered it is relatively easy to tease out the role of the explanatory variables that have been included in one or alternative versions of the regression equation. Especially where there are three or more explanatory variables with the manner in which they should enter into the regression equation is unclear the effect of an individual variable that is of interest can be diﬀicult or impossible to tease out. The hillrace dataset has record times for  Northern Ireland Mountain Running Association hillraces as given in the  calendar. In the models fitted and graphs shown that follow the distance measure is Dist (distance converted to kilometers) the climb measure is Climb (vertical distance between lowest and highest point in meters) and the time measure is Time (in minutes).
How does time per unit distance (timePerKm) vary with distance. We will fit two equations both with 𝑦 timePerKm. A Climb held constant at mean value B log(ClimbDist) held constant at mean value Minutes per kilometer  Minutes per kilometer          Dist Dist Figure  Variation in time per unit distance with distance. Panel A shows the pattern of change when log(Climb) is held constant at its mean value while Panel B shows the pattern of change when log(ClimbDist) is held constant at its mean value. Figure 8.6A shows the dependence of timePerKm depends on log(Dist) when log(Climb) is held at its mean value. Use of log(Dist) rather than Dist means that distance on the 𝑥axis from  to  (km) is the same as from  to  or from  to  or from  to  i.e. equal distances correspond to equal multiplicative changes. The equation that is plotted is timePerKm   log(Dist) Figure 8.6B shows the dependence of timePerKm depends on log(Dist) when log(ClimbDist) is held at its mean value. The equation that is plotted is timePerKm   log(Dist) In Panel A time per kilometer decreases quite sharply as distance increases. This happens because the ratio of Climb to Dist decreases if Climb is held constant while Dist increases i.e. longer distance races involve gentler ascents and descents. Panel B shows what happens when ClimbDist is held constant i.e. we are comparing races with the same ratio of Climb to Dist. As expected time per
kilometer does then decrease as distance increases.  Model that do not correctly fit the data readily mis lead Are herricanes more dangerous than himmicanes US Atlantic Hurricanes  to  Females Males  Dashed lines red for femaleand blue for male Katrina  are close equivalents to the Jung et al ﬁts.  Audrey  Deaths           Damage at the time (millions of  US) Figure  Deaths versus damage estimate in US dollars. The red (for female) and blue (for male) dashed lines are close equivalents of Jung at als fit to the data. The 𝑦axis uses a scale of equal relative numbers of deaths while the 𝑥axis uses a scale of equal dollar damage costs. The United States National Hurricane Center began formally naming hurricanes in  a task now under control of the World Meteorological Organization. Female names were used for Atlantic hurricanes from  to  with a mix of male and female names used from  onwards. In a paper titled Female hurricanes are deadlier than male hurricanes Jung et al. (2014) used data for  Atlantic hurricanes that made landfall in the United States during  to argue that death rates from those with female names were overall higher than for those with male names. The suggestion
was that where names were female authorities took the risk less seriously. The paper attracted wide interest on the blogosphere with female hurricanes jokingly called herricanes and males called himmicanes. US Atlantic Hurricanes  to  Females Males  Dashed curves translate the male Katrina  female regression lines in the previous  ﬁgure to the present graph. The solid curves show a databased relationship. Audrey  Deaths           1e05 Damage at the time (millions of  US) Figure  Deaths versus damage estimate in US dollars with logarithmic scales on both axes. Separate fitted lines for male and female hurricanes cannot be distinguished. Jung et al used a logarithmic scale on the vertical axis only which on this graph leads to the dashed curves. The separate dotted lines in Figure  red for female and blue for male are a close equivalent to the authors fit to the data. Notice the use of a relative (numbers of deaths) scale on the 𝑦axis and a dollar scale on the 𝑥axis. An unfortunate consequence of the use of a linear dollar scale on the 𝑥axis is that the slopes of the lines are strongly influenced by the final four points at the upper end of the scale. Why did the authors not use at least as a starting point the same relative scale on both axes as in Figure  As well as using a relative scale on the 𝑥axis Figure  uses a methodology that allows the data to determine the form of the response. Deaths do on average increase more at a higher rate than the damage measure but not at the rate suggested by the dashed curves. There is now no evident difference between the two curves.
Jung et al omitted Audrey (in 1957) and Katrina (in 2005) as outliers. These are included in Figures  and  with the curves fitted using a robust fitting method that is relatively insensitive to outliers. Other differences between the Jung et al analysis and the analyses reflected in Figures  and  are documented in Note  on p.  Historical speed of light estimates is there a pattern A All measurements B From  with conﬁdence bands Speed (1000s of kms)              Figure  Successive speed of light estimates. Panel B limits attention to measurements made in  and later. The line was fitted with no adjustment for the very different error estimates. The dashed curve which incorporates such adjustments is statistically indistinguishable from the thick gray horizontal line. Creationist Barry Setterfield has argued that a reduction over time in the speed of light has led the passage of time to slow down relative to the remote past so that the universe is thousands rather than billions of years old. His arguments rely on making various adjustments to figures obtained historically selecting what he regarded as the most reliable data and then fitting a curve. Setterfield tells a story that while a little different from that of the line in Panel A of Figure  makes equally little sense. The right panel is limited to the points from  and on marked off with the gray background on the left panel.6 For the measurements from  onward estimates of accuracy are available. Until  each new estimate lay outside the bounds for the previous estimate 6Data are from httpsen.wikipedia.orgwikiSpeedoflight
    indicating that these were underestimates. Even if one were to accept Setterfields manipulation of the data it makes no sense at all to fit either lines such as are shown or curves to data values which have such very different accuracies. Even if one were to accept Setterfields manipulation of the data it makes no sense at all to fit either lines such as are shown or curves to data values which have such very different accuracies as those shown in the graphs. For the measurements from  onwards estimates of accuracy are available. Until  each new estimate lay outside the bounds for the previous estimate indicating that these were underestimates. 8.7.1 Global mean temperature trends Figure  plots global air and sea surface temperature anomaly data against year. Anomalies in hundredths of a degree centigrade are differences from the  global average. The grey curve plots the average anomaly up to that point in time. Global temperature differences from  global average         Year Figure  Anomalies (differences) in hundredths of a degree centigrade from global average temperatures over  plotted against year. The gray curve shows for each year the average anomaly up to that point in time. The last year in which this lay below the gray line was 1962. Observe that  was the last year in which the global temperature fell below
the average to that time. For the  subsequent years (from  to  inclusive) the global average was above the average up to that date. Under the (false) assumption that global temperature is varying randomly (and therefore independently) about a common mean the probability of this happening is   1013. A variation of this argument came from a speaker on the Australian ABC Science Show on April  2011. Under any model that accounts for what are now fairly well understood patterns of correlation over time the probability while very small is not that small Arguments that overstate the case for what is now a wellestablished pattern of change are unhelpful It is likewise nonsensical to fit a line to the cherrypicked years  where the trend was relatively flat. Year to year temperatures are correlated.
Critiquing scientific claims To be credible scientific claims must be able to survive informed criticism. The nature of the critique that is needed will vary depending on what is in question. It will vary at a broader level depending on whether what is in question is Measurements e.g. as in astronomy distances to other planets stars and galaxies or the results of an experiment e.g. showing that plants grow better when compost is added to a nutrient deficient soil or a theory such as Newtons law of gravity or laws of motion. A theory is a model that is designed to describe natural phenom ena. A theory in conceptual relying on entities such as electrons and waves that lie outside of normal human experience where a law is a mathematical description of observable phenomena. Large parts of science and scientific applications rely on models that in turn build on scientific theorieslaws. Epidemiological models have been crucial to informing New Zealands response to the Covid19 pandemic. Climate models are an example of very complex models. These rely on The basic laws of physics fluid motion and chemistry Knowledge of the way that atmosphere oceans land surface ice and solar radiation interact to change climate. Computer simulation to build in the effects of areas of uncertainty. Measurements and experimental results must be replicable i.e. another ex 
perimenter must be able to repeat the experiment and obtain the same results. Theories must be able to make successful predictions. In areas where results depend on the sharing of data and skills between different scientists and groups of scientists the critique that authors provide to the work of their fellow authors will commonly ensure that what is submitted for peer review is soundly based. For experimental studies that are designed to stand on their own the past decade has seen the emergence of concerning evidence that a large amount of published work is when put to the test not replicable. The steps needed to implement change are well understood. The slow pace of reform is disappointing. More generally uncritical and faulty statistical analyses such as have been documented earlier in this book are a cause for concern. Too often model fitting becomes a ritual without the use of standard types of diagnostic checks that would have demonstrated that the model was faulty. Stark and Saltelli (2018) comment The mechanical ritualistic application of statistics is contributing to a crisis in science. Education software and peer review have encouraged poor practiceand it is time for statisticians to fight back. . . . The problem is one of cargocult statistics the ritualistic miming of statistics rather than conscientious practice. This has become the norm in many disciplines reinforced and abetted by statistical education statistical software and editorial policies. It is not just statisticians who should be fighting back but all scientists who care about the public credibility of scientific processes.  What results can be trusted Scientific processes work best when claims made by one scientist or group of scientists attract widespread interest and critique from a wider group of scientists who understand the work well enough to provide informed and incisive criticism. This can be an effective way to identify claims that have no sound basis. Examples are the May  Lancet and New England Journal of Medicine studies claiming to be based on observational data arguing that use of the drug hydroxychloroquine as a treatment for Covid19 was increasing patient deaths. Issues with these papers were quickly identified because they made claims that bore on an issue of major concern and attracted attention from readers who
carefully scrutinized their detailed statements. They were quickly retracted. How much that has no sound basis does that attract such attention and is never challenged Heavy reliance on the sharing of data and skills and full use of the benefits that modern technology has to offer have been vital to progress in such areas as earthquake science the study of viruses and vaccines modelling of epidemics and climate science. This sharing of data and skills and use of modern technology also helps in the critique of what has been published earlier. Areas where there has not been the same impetus for change are much more susceptible to the damage that arises from systems for funding and publishing science that encourage the formal publication of what would better be treated as preliminary results a first stab at an answer. Publication of experimental results should not be a onceforall event but a staged process that moves from this looks promising to has been independently replicated and to postpublication critique. Publication does not of itself validate scientific claims Rather as stated in Popper (1963) Observations or experiments can be accepted as supporting a theory (or a hypothesis or a scientific assertion) only if these observations or experiments are severe tests of the theory or in other words only if they result from serious attempts to refute the theory. The demand that experimental results are shown to be replicable is central to effective scientific processes. As Fisher (1937) wrote No isolated experiment however significant in itself can suﬀice for the experimental demonstration of any natural phenomenon . . . In order to assert that a natural phenomenon is experimentally demon strable we need not an isolated record but a reliable method of procedure. Sources of failure Fraud though uncommon happens more often than one might hope. What is disturbing is the small number of scientists with large numbers of papers that were retracted on account of fraud. How were they able to get away with publishing so many papers usually with fraudulent data before the first identification of fraud that led to a checking of all their work Ritchie (2020 6768))
cites as an extreme example the case of a Japanese anesthesiologist with  retracted papers. More common are mistakes in data collection unacknowledged sources of bias hype mistakes or biases in the handling of data andor in data analysis attaching a much higher degree of certainty to statistical evidence than the results warrant and selection effects. What gets published can be strongly affected by selection effects. There may be selection of a subset of data where there appears to be an effect of interest choice of the outcome variable andor analysis approach that most nearly gives the result that is wanted and so on. In analysis of data from experiments where two treatments are compared the common use of the arbitrary 𝑝  criterion as a cutoff for deciding what will be published has the inevitable effect of selecting out for publication one in twenty (or more) results where there was no difference of consequence.1  The case of Eysenck and his collaborators At the time of his death in  Eysenck was the living psychologist most frequently cited in the peerreviewed scientific literature. Much of his work was controversial in its time with papers containing questionable data and results so dramatic they beggared belief (OGrady 2020). He relied heavily on what has now been identified as heavily doctored data that was supplied to him by German collaborator GrossarthMaticek. Particularly egregious was the claim that individuals with an identifiably cancerprone personality had a risk of dying from cancer that was as much as  times higher than that of people with a healthy personality one of several links that the duo claimed to have found between personality and mortality. Investigations into Eysencks work including collaborative work with GrossarthMaticek are ongoing. Fourteen papers have been retracted and another  have received expressions of concern. A large replication study conducted in  found none of the claimed links apart from a modest link between personality and cardiovascular disease.2 In a book published two years before his death Eysenck made comments that provide an intriguing insight into his thinking. 1Note  (p. 89) in the notes at the end of the book makes relatively technical comments on common misunderstandings that affect the use of 𝑝values. 2See further Craig Pelosi and Tourish (2021).
Scientists have extremely high motivation to succeed in discovering the truth their finest and most original discoveries are rejected by the vulgar mediocrities filling the ranks of orthodoxy. .. . . The figures dont quite fit so why not fudge them a little bit to confound the infidels and unbelievers Usually the genius is right of course and we may in retrospect excuse his childish games but clearly this cannot be regarded as a licence for nongeniuses to foist their absurd beliefs on us. (Eysenck  197) Eysenck was alluding to claims that Newton had manipulated data and suggesting that it was excusable for other geniuses to do the same.  Detection of Covid19 from chest images Roberts et al. (2021) identified an astonishing  papers and preprints that appeared between  January  to  October  and which describe the use of new machine learning models for the diagnosis of COVID19 from chest radiographic (CXR) and chest computed tomography (CT) images. Quality screening reduced this number to  which were then examined in more detail. None of the  satisfied these more detailed requirements designed to check whether the algorithms used had been shown to be effective for use in clinical practice. Among other deficiencies  did not complete any external validation and  had a high risk of bias with respect to at least one of participants predictors outcomes and analysis. A significant number of systems were trained on Xrays from adults with covid19 and children without so that they were likely to detect whether the Xray was from adult or child rather than whether the person had Covid19. In an account of the results that appeared in New Scientist Roberts (2021) comments that relative to persisting to develop a model that will survive a rigorous checking process and might be used in practice it is far easier to develop a model with poor rigour and apparent excellent performance and publish this. This is a damning indictment of the way that large parts of the research and publication process currently work. The public good would be much better served by a process that encourages researchers to persist until it has been demonstrated that researchers have a model that meets standards such as are set out in Roberts (2021).
 Laboratory studies what do we find In the past several years there has been a steady accumulation of evidence that relates to the claim in Ioannidis (2005) that most published research findings are false. Ioannidis has in mind not published results in general but primarily laboratory studies. Papers that have added to the body of evidence that broadly support claims made in the Ioannidis paper include Amgen Reproduced  only of  landmark cancer studies. Begley and Ellis (2012) Begley (2013) notes issues with the studies that failed Bayer Main results from  of  seminal drug studies NB journal impact factor was not a good predictor Prinz Schlange and Asadullah (2011) fMRI studies  of  papers (42) had  case lacking check on separate test image. Another  unclear Kriegeskorte et al. (2009) Issues that Begley (2013) notes are important both across the preclinical studies such as are his concern and for clinical studies such as have been used and are being used the check the effectiveness and side effects of Covid19 vaccines. Begley comments that the flaws many of the flaws that he identified were identified and expunged from clinical studies decades ago. They include Were experiments blinded Was recording of all results done by another investigator who did not know what treatment had been applied Were basic counts measurements and tests such as are done using West ern blotting (used to detect specific proteins in a mixture) repeated Were all results presented One concern is that when images are shown (e.g. of Western blot gels) images may be cropped in ways that more clearly identify the protein than is really the case. Were results shown for crucial control experiments Readers should be provided with evidence that there was unlikely to be any bias in the comparison of treatment with control. Were reagents validated Reagents must be shown to be fit for purpose with results shown from analyses that validate their use. Were statistical tests appropriate
Begley comments on common analysis flaws. Begley comments What is also remarkable is that many of these flaws were identified and expunged from clinical studies decades ago. In such studies it is now the gold standard to blind investigators include concurrent controls rigorously apply statistical tests and analyse all patients we cannot exclude patients because we do not like their outcomes. Positive developments in the psychology community The psychological science community is further advanced in addressing these issues that many other communities with The Center for Open Science (COS) taking a strong lead in studies designed to document the extent of the issues. Other Center for Open Science (COS) Projects have included The Reproducibility Psychology project. This replicated  studies from  journals in cognitive and social psychology. In order to keep the graph simple Figure  limits attention to the subset that relate to social psychology. OSC (2015) Many Labs reproduce  classical psych studies Of  studies  successful  weakly  no Plots show scatter across the  participating teams Klein et al. (2014) Cancer Studies  most impactful from  Kaiser (2015) Details from other relevant studies are given in the recent book Ritchie (2020) Science fictions Exposing fraud bias negligence and hype in science. In the areas of science to which these studies relate it is then clear that published claims that have not been replicated are as likely as not to be false. As Begley (2013) notes however there are clues that can allow a discerning to make a judgement on the credibility that should be given to claims that are made. Figure  compares the effect size for the replicate with the effect size for the original for the  social psychology studies included in the Reproducibility Psychology project.
Reproducibility Psychology Social Science    JPSP PS         Effect size r (original) Figure  Psychology reproducibility project. Effect sizes are compared between the replication and the initial study for the  social psychology studies included in the Reproducibility Psychology project. The journals that are represented are Psychological Science (PS) and Journal of Personality and Social Psychology (JPSP) The effect size is a measure of the average difference found divided by an estimate of variability for the individual results. The effect size is smaller for the replication in  out of the  studies. A smooth curve with confidence interval has been fitted. It is only for an original effect size greater than  that one starts to see a positive correlation between the effect sizes for the replicate and that for the original. Other replication studies The critiques have limited relevance to areas where the nature of the work forces collaboration between scientists with diverse skills widely across different research groups. Where all data and code used in modelling are out in the open for all to see and evaluate and research requires cooperation internationally across multiple areas of expertise illfounded or exaggerated claims are unlikely to survive long. Differences in the extent to which the nature of the work force cooperation go a long way to explaining the large variation that is evident in

